<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>\method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs</title>
<!--Generated on Fri Jul 12 14:32:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.07775v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S1" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S2" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title">Classical Navigation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title">Object and Visual Navigation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title">Vision-language models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S3" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MINT Problem Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S4" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_ERROR undefined">\method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S4.SS1" title="In 4 \method ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Demonstration Tour and Offline Topological Graph Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S4.SS2" title="In 4 \method ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>High-Level Goal Finding with Long-Context Multimodal VLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S4.SS3" title="In 4 \method ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Low-level Goal Reaching using Topological Graphs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.SS1" title="In 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span><span class="ltx_text ltx_font_bold">RQ1:</span> <span class="ltx_ERROR undefined">\method</span>’s robust high end-to-end performance in the wild</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.SS2" title="In 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>RQ2: Long-context VLM outperforms alternatives on high level goal finding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.SS3" title="In 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>RQ3: Topological graphs are critical for success</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S6" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7" title="In \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS1" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Structure-from-Motion and Hierarchical Localization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS2" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Home-Like Environment Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS3" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Additional Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS4" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Large-Scale end-to-end sim details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS5" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>Full VLM intermediate output in Figure <span class="ltx_text ltx_ref_tag">3</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS6" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Prompt for direct VLM waypoint actions output</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS7" title="In 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>User Instructions tested in the Office environment</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_ERROR undefined" id="id2.id1">\method</span>: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hao-Tien Lewis Chiang &amp;Zhuo Xu<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> &amp;Zipeng Fu<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> &amp;Mithun George Jacob &amp;Tingnan Zhang<span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span> &amp;Tsang-Wei Edward Lee<span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span> &amp;Wenhao Yu<span class="ltx_note ltx_role_footnotemark" id="footnotex5"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span> &amp;Connor Schenck &amp;David Rendleman &amp;Dhruv Shah &amp;Fei Xia &amp;Jasmine Hsu &amp;Jonathan Hoech &amp;Pete Florence &amp;Sean Kirmani &amp;Sumeet Singh &amp;Vikas Sindhwani &amp;Carolina Parada &amp;Chelsea Finn<span class="ltx_note ltx_role_footnotemark" id="footnotex6"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span> &amp;Peng Xu<span class="ltx_note ltx_role_footnotemark" id="footnotex7"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span> &amp;Sergey Levine<span class="ltx_note ltx_role_footnotemark" id="footnotex8"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span> &amp;Jie Tan<span class="ltx_note ltx_role_footnotemark" id="footnotex9"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span> <span class="ltx_ERROR undefined" id="id3.1.id1">\AND</span>Google DeepMind
</span><span class="ltx_author_notes">Co-first authorsCore contributorsAdvising leadsCorresponding authors:{<span class="ltx_text ltx_font_typewriter" id="id4.2.id1">lewispro, zhuoxu, jietan</span>}<span class="ltx_text ltx_font_typewriter" id="id5.3.id2">@google.com</span>. Zipeng completed his part of work while at Google DeepMind.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">An elusive goal in navigation research is to build an intelligent agent that can understand multimodal instructions including natural language and image, and perform useful navigation.
To achieve this, we study a widely useful category of navigation tasks we call Multimodal Instruction Navigation with demonstration Tours (MINT), in which the environment prior is provided through a previously recorded demonstration video.
Recent advances in Vision Language Models (VLMs) have shown a promising path in achieving this goal as it demonstrates capabilities in perceiving and reasoning about multimodal inputs.
To solve MINT, we present <span class="ltx_ERROR undefined" id="id1.1.1">\method</span>, a hierarchical Vision-Language-Action (VLA) navigation policy that combines the environment understanding and common sense reasoning power of long-context VLMs and a robust low-level navigation policy based on topological graphs.
The high-level policy consists of a long-context VLM that takes the demonstration tour video and the multimodal user instruction as input to find the goal frame in the tour video.
Next, a low-level policy uses the goal frame and an offline constructed topological graph to generate robot actions at every timestep.
We evaluated <span class="ltx_ERROR undefined" id="id1.1.2">\method</span> in a 836m<sup class="ltx_sup" id="id1.1.3">2</sup> real world environment and show that <span class="ltx_ERROR undefined" id="id1.1.4">\method</span> has a high end-to-end success rates on previously unsolved multimodal instructions such as “Where should I return this?” while holding a plastic bin.
A video demonstrating <span class="ltx_ERROR undefined" id="id1.1.5">\method</span> can be found here: <a class="ltx_ref ltx_href" href="https://youtu.be/-Tof__Q8_5s" title="">youtu.be/-Tof__Q8_5s</a></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<blockquote class="ltx_quote" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Keywords:</span> vision-language navigation, multimodal foundation models, long-context reasoning</p>
</blockquote>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="301" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_ERROR undefined" id="S0.F1.3.1">\method</span> architecture. The multimodal user instruction and a demonstration tour video of the environment are used by a long-context VLM (high-level policy) to identify the goal frame in the video. The low-level policy then uses the goal frame and an <span class="ltx_text ltx_font_italic" id="S0.F1.4.2">offline</span> generated topological map (from the tour video using structure-from-motion) to compute a robot action at every timestep.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Robot navigation has come a long way.
Early work relied on users specifying physical coordinates in pre-mapped environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib7" title="">7</a>]</cite>.
Object goal and Vision Language navigation (ObjNav and VLN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib14" title="">14</a>]</cite> are a giant leap forward in robot usability as they allow the use of open-vocabulary language to define navigation goals, such as “Go to the couch”.
To make robots truly useful and ubiquitous in our daily lives, we propose another leap forward by lifting ObjNav and VLN’s natural language space onto the multimodal space, meaning that the robot can accept natural language and/or image instructions simultaneously. For example, a person unfamiliar with the building can ask “Where should I return this?” while holding a plastic bin (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S0.F1" title="Figure 1 ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a>, upper left), and the robot guides the user to the shelf for returning the box based on verbal and visual context. We call this category of navigation tasks Multimodal Instruction Navigation (MIN).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">MIN is a broad task consisting of environment exploration and instruction guided navigation. However, in many scenarios one can bypass exploration by leveraging a <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">demonstration tour video</span> that fully traverses the environment. The demonstration tour has several benefits: 1) It is easy to collect: users can teleoperate the robot or simply record a video on a smartphone while walking in the environment. There also exists exploration algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib14" title="">14</a>]</cite> that can be used to create the tour. 2) It aligns with user common practice: when a user gets a new home robot, it is natural for them to show the robot around in their home, and they can verbally introduce locations of interest during the tour. 3) In certain circumstances, restricting the robot’s motion in a pre-defined zone is desirable due to safety and privacy purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib15" title="">15</a>]</cite>. To that end, in this paper, we introduce and study this category of tasks called Multimodal Instruction Navigation with Tours (MINT), which leverages demonstration tours and focuses on fulfilling multimodal user instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recently, large Vision-Language Models (VLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib18" title="">18</a>]</cite> have shown great potential in solving MINT thanks to their impressive capabilities in language and image understanding and common-sense reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib19" title="">19</a>]</cite>, all critical pieces to achieve MINT.
However, VLMs alone struggle to solve MINT due to: 1) The number of input images for many VLMs are highly limited due to context-length limitation.
This severely limits the fidelity of environment understanding in large environments.
2) Solving MINT requires computing robot actions.
Queries to solicit such robot actions are typically out-of-distribution from what VLMs are (pre)trained with.
As a result, the zero-shot navigation performance is often unsatisfactory (we show this in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.SS3" title="5.3 RQ3: Topological graphs are critical for success ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To solve MINT, we present <span class="ltx_ERROR undefined" id="S1.p4.1.1">\method</span>, a hierarchical Vision-Language-Action (VLA) navigation policy that combines the environment understanding and common sense reasoning power of <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">long-context</span> VLMs and a robust low-level navigation policy based on topological graphs.
Specifically, the high-level VLM uses the demonstration tour video and the multimodal user instruction to find the goal frame in the tour video.
Next, a classical low-level policy uses the goal frame and a topological graph (constructed offline from tour frames) to generate robot actions (waypoints) at every timestep.
The use of long-context VLMs addressed the environment understanding fidelity problem, and the topological graph bridges the gap between VLM’s training distribution and the robot actions required to solve MINT.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We evaluated <span class="ltx_ERROR undefined" id="S1.p5.1.1">\method</span> in a real world (836<math alttext="m^{2}" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><msup id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml">m</mi><mn id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1">superscript</csymbol><ci id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2">𝑚</ci><cn id="S1.p5.1.m1.1.1.3.cmml" type="integer" xref="S1.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">m^{2}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>) office and a home-like environment. <span class="ltx_ERROR undefined" id="S1.p5.1.2">\method</span> achieved 86% and 90% success rates (26% and 60% higher than baseline methods) on previously infeasible MINT tasks involving complex reasoning (e.g., “I want to store something out of sight from the public eye. Where should I go?”) and multimodal user instructions.
We also demonstrated a leap forward in how easily users can interact with the robot,
where a user takes a narrated video walkthrough in a home environment <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">with a smartphone</span> and then asks “Where did I leave my coaster?”</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our contributions include:
1) proposed a new paradigm of robot navigation: MIN and its variant MINT, which make robots significantly more helpful and intuitive to use.
2) proposed <span class="ltx_ERROR undefined" id="S1.p6.1.1">\method</span> as a solution of MINT, which combines long-context VLMs and topological maps. This method has significantly improved the naturalness of human-robot interaction, and greatly increased the robot usability.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Classical Navigation</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Classical navigation methods typically focus on moving the robot from point-to-point, where goals are specified in metric coordinates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib20" title="">20</a>]</cite>. These systems usually rely on pre-built or dynamically generated maps and utilize path-planning algorithms like D* <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib21" title="">21</a>]</cite> to generate fine-grained navigation commands (e.g., twist drive velocity) to achieve collision-free movement. Similar to previous works, the low-level controller used in this work combines a visual SLAM algorithm, COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib22" title="">22</a>]</cite>, and an iLQR based Model Predictive Control (MPC) method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib23" title="">23</a>]</cite> to track desired waypoints obtained from high-level VLMs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Object and Visual Navigation</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">While classical navigation methods typically exhibit robust behavior, they do not leverage semantically meaningful information for specifying navigation targets. In contrast, object and image goal navigation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib26" title="">26</a>]</cite> utilize rich input modalities. These include object categories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib11" title="">11</a>]</cite>, natural language instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib31" title="">31</a>]</cite>, dialogue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib32" title="">32</a>]</cite>, goal image conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib34" title="">34</a>]</cite>, and even multimodal inputs combining language and images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">Most of these approaches involve an active exploration phase because the robot operates without prior knowledge of the environment. Our work distinguishes itself by leveraging environment priors provided in the form of a previously collected video tour. In this regard, our work shares similarities with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib14" title="">14</a>]</cite>, where semantic information is obtained from past explorations, and  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib38" title="">38</a>]</cite> which utilizes memory to improve mapping and planning. However, a key difference lies in the absence of explicit semantic scene representation graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib40" title="">40</a>]</cite> in our approach, thanks to the capabilities of VLMs to process raw videos.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Vision-language models</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Prior to the emergence of large VLMs, researchers typically needed to pretrain their own visual representations for navigation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib44" title="">44</a>]</cite>, although some leveraged existing pretrained multimodal embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib14" title="">14</a>]</cite>. Recent breakthroughs in large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib46" title="">46</a>]</cite> and VLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib18" title="">18</a>]</cite>, trained on web-scale data, have paved the way for zero or few-shot navigation capabilities. This potential has been explored in various studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib50" title="">50</a>]</cite>, showcasing the diverse applications of LLMs and VLMs in navigation. These models have demonstrated the ability to: Provide navigation preferences, e.g., “stay close to marked pavements” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib48" title="">48</a>]</cite>; Construct high-level motion plans, e.g., “move past the hallway towards the bedroom” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib49" title="">49</a>]</cite>; Substitute object detectors, i.e., recognizing landmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib50" title="">50</a>]</cite>; In some cases directly output trajectories  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib51" title="">51</a>]</cite>. Our work is most similar to  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib49" title="">49</a>]</cite> in the sense that a large VLM (Gemini Pro 1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib17" title="">17</a>]</cite>) is used to generate high-level navigation plans for the robot, but differs from the the previous work in that our VLM directly outputs the navigation goal for the low-level controller to consume and generate navigation commands.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MINT Problem Formulation</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The MINT task considered in this paper takes as input a demonstration tour video and a multimodal user instruction.
The robot must navigate to certain goal location(s) to satisfy the user’s instruction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.5">Under this setting, the demonstration tour video consists of a sequence of first-person view image frames <math alttext="F=\{f_{i}|f_{i}\in\mathbb{R}^{H\times W\times 3},i=1,2,...,k\}" class="ltx_Math" display="inline" id="S3.p2.1.m1.6"><semantics id="S3.p2.1.m1.6a"><mrow id="S3.p2.1.m1.6.6" xref="S3.p2.1.m1.6.6.cmml"><mi id="S3.p2.1.m1.6.6.4" xref="S3.p2.1.m1.6.6.4.cmml">F</mi><mo id="S3.p2.1.m1.6.6.3" xref="S3.p2.1.m1.6.6.3.cmml">=</mo><mrow id="S3.p2.1.m1.6.6.2.2" xref="S3.p2.1.m1.6.6.2.3.cmml"><mo id="S3.p2.1.m1.6.6.2.2.3" stretchy="false" xref="S3.p2.1.m1.6.6.2.3.1.cmml">{</mo><msub id="S3.p2.1.m1.5.5.1.1.1" xref="S3.p2.1.m1.5.5.1.1.1.cmml"><mi id="S3.p2.1.m1.5.5.1.1.1.2" xref="S3.p2.1.m1.5.5.1.1.1.2.cmml">f</mi><mi id="S3.p2.1.m1.5.5.1.1.1.3" xref="S3.p2.1.m1.5.5.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.1.m1.6.6.2.2.4" lspace="0em" rspace="0em" xref="S3.p2.1.m1.6.6.2.3.1.cmml">|</mo><mrow id="S3.p2.1.m1.6.6.2.2.2.2" xref="S3.p2.1.m1.6.6.2.2.2.3.cmml"><mrow id="S3.p2.1.m1.6.6.2.2.2.1.1" xref="S3.p2.1.m1.6.6.2.2.2.1.1.cmml"><msub id="S3.p2.1.m1.6.6.2.2.2.1.1.2" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2.cmml"><mi id="S3.p2.1.m1.6.6.2.2.2.1.1.2.2" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2.2.cmml">f</mi><mi id="S3.p2.1.m1.6.6.2.2.2.1.1.2.3" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S3.p2.1.m1.6.6.2.2.2.1.1.1" xref="S3.p2.1.m1.6.6.2.2.2.1.1.1.cmml">∈</mo><msup id="S3.p2.1.m1.6.6.2.2.2.1.1.3" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.cmml"><mi id="S3.p2.1.m1.6.6.2.2.2.1.1.3.2" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.cmml"><mi id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.2" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.2.cmml">H</mi><mo id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.1.cmml">×</mo><mi id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.3" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.3.cmml">W</mi><mo id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.1.cmml">×</mo><mn id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.4" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><mo id="S3.p2.1.m1.6.6.2.2.2.2.3" xref="S3.p2.1.m1.6.6.2.2.2.3a.cmml">,</mo><mrow id="S3.p2.1.m1.6.6.2.2.2.2.2" xref="S3.p2.1.m1.6.6.2.2.2.2.2.cmml"><mi id="S3.p2.1.m1.6.6.2.2.2.2.2.2" xref="S3.p2.1.m1.6.6.2.2.2.2.2.2.cmml">i</mi><mo id="S3.p2.1.m1.6.6.2.2.2.2.2.1" xref="S3.p2.1.m1.6.6.2.2.2.2.2.1.cmml">=</mo><mrow id="S3.p2.1.m1.6.6.2.2.2.2.2.3.2" xref="S3.p2.1.m1.6.6.2.2.2.2.2.3.1.cmml"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">1</mn><mo id="S3.p2.1.m1.6.6.2.2.2.2.2.3.2.1" xref="S3.p2.1.m1.6.6.2.2.2.2.2.3.1.cmml">,</mo><mn id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">2</mn><mo id="S3.p2.1.m1.6.6.2.2.2.2.2.3.2.2" xref="S3.p2.1.m1.6.6.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.p2.1.m1.3.3" mathvariant="normal" xref="S3.p2.1.m1.3.3.cmml">…</mi><mo id="S3.p2.1.m1.6.6.2.2.2.2.2.3.2.3" xref="S3.p2.1.m1.6.6.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.p2.1.m1.4.4" xref="S3.p2.1.m1.4.4.cmml">k</mi></mrow></mrow></mrow><mo id="S3.p2.1.m1.6.6.2.2.5" stretchy="false" xref="S3.p2.1.m1.6.6.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.6b"><apply id="S3.p2.1.m1.6.6.cmml" xref="S3.p2.1.m1.6.6"><eq id="S3.p2.1.m1.6.6.3.cmml" xref="S3.p2.1.m1.6.6.3"></eq><ci id="S3.p2.1.m1.6.6.4.cmml" xref="S3.p2.1.m1.6.6.4">𝐹</ci><apply id="S3.p2.1.m1.6.6.2.3.cmml" xref="S3.p2.1.m1.6.6.2.2"><csymbol cd="latexml" id="S3.p2.1.m1.6.6.2.3.1.cmml" xref="S3.p2.1.m1.6.6.2.2.3">conditional-set</csymbol><apply id="S3.p2.1.m1.5.5.1.1.1.cmml" xref="S3.p2.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.5.5.1.1.1.1.cmml" xref="S3.p2.1.m1.5.5.1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.5.5.1.1.1.2.cmml" xref="S3.p2.1.m1.5.5.1.1.1.2">𝑓</ci><ci id="S3.p2.1.m1.5.5.1.1.1.3.cmml" xref="S3.p2.1.m1.5.5.1.1.1.3">𝑖</ci></apply><apply id="S3.p2.1.m1.6.6.2.2.2.3.cmml" xref="S3.p2.1.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.1.m1.6.6.2.2.2.3a.cmml" xref="S3.p2.1.m1.6.6.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.p2.1.m1.6.6.2.2.2.1.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1"><in id="S3.p2.1.m1.6.6.2.2.2.1.1.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.1"></in><apply id="S3.p2.1.m1.6.6.2.2.2.1.1.2.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.p2.1.m1.6.6.2.2.2.1.1.2.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2">subscript</csymbol><ci id="S3.p2.1.m1.6.6.2.2.2.1.1.2.2.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2.2">𝑓</ci><ci id="S3.p2.1.m1.6.6.2.2.2.1.1.2.3.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.p2.1.m1.6.6.2.2.2.1.1.3.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.p2.1.m1.6.6.2.2.2.1.1.3.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3">superscript</csymbol><ci id="S3.p2.1.m1.6.6.2.2.2.1.1.3.2.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.2">ℝ</ci><apply id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3"><times id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.1"></times><ci id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.2.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.2">𝐻</ci><ci id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.3.cmml" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.3">𝑊</ci><cn id="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.4.cmml" type="integer" xref="S3.p2.1.m1.6.6.2.2.2.1.1.3.3.4">3</cn></apply></apply></apply><apply id="S3.p2.1.m1.6.6.2.2.2.2.2.cmml" xref="S3.p2.1.m1.6.6.2.2.2.2.2"><eq id="S3.p2.1.m1.6.6.2.2.2.2.2.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.2.2.1"></eq><ci id="S3.p2.1.m1.6.6.2.2.2.2.2.2.cmml" xref="S3.p2.1.m1.6.6.2.2.2.2.2.2">𝑖</ci><list id="S3.p2.1.m1.6.6.2.2.2.2.2.3.1.cmml" xref="S3.p2.1.m1.6.6.2.2.2.2.2.3.2"><cn id="S3.p2.1.m1.1.1.cmml" type="integer" xref="S3.p2.1.m1.1.1">1</cn><cn id="S3.p2.1.m1.2.2.cmml" type="integer" xref="S3.p2.1.m1.2.2">2</cn><ci id="S3.p2.1.m1.3.3.cmml" xref="S3.p2.1.m1.3.3">…</ci><ci id="S3.p2.1.m1.4.4.cmml" xref="S3.p2.1.m1.4.4">𝑘</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.6c">F=\{f_{i}|f_{i}\in\mathbb{R}^{H\times W\times 3},i=1,2,...,k\}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.6d">italic_F = { italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × 3 end_POSTSUPERSCRIPT , italic_i = 1 , 2 , … , italic_k }</annotation></semantics></math> taken during a tour of the environment, where <math alttext="k" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_k</annotation></semantics></math> is the number of frames in the video.
In addition, optional natural language narratives can be added to certain frames <math alttext="N=\{n_{j}|n_{j}\in\texttt{str},j\in[1,2,...,k]\}" class="ltx_Math" display="inline" id="S3.p2.3.m3.6"><semantics id="S3.p2.3.m3.6a"><mrow id="S3.p2.3.m3.6.6" xref="S3.p2.3.m3.6.6.cmml"><mi id="S3.p2.3.m3.6.6.4" xref="S3.p2.3.m3.6.6.4.cmml">N</mi><mo id="S3.p2.3.m3.6.6.3" xref="S3.p2.3.m3.6.6.3.cmml">=</mo><mrow id="S3.p2.3.m3.6.6.2.2" xref="S3.p2.3.m3.6.6.2.3.cmml"><mo id="S3.p2.3.m3.6.6.2.2.3" stretchy="false" xref="S3.p2.3.m3.6.6.2.3.1.cmml">{</mo><msub id="S3.p2.3.m3.5.5.1.1.1" xref="S3.p2.3.m3.5.5.1.1.1.cmml"><mi id="S3.p2.3.m3.5.5.1.1.1.2" xref="S3.p2.3.m3.5.5.1.1.1.2.cmml">n</mi><mi id="S3.p2.3.m3.5.5.1.1.1.3" xref="S3.p2.3.m3.5.5.1.1.1.3.cmml">j</mi></msub><mo id="S3.p2.3.m3.6.6.2.2.4" lspace="0em" rspace="0em" xref="S3.p2.3.m3.6.6.2.3.1.cmml">|</mo><mrow id="S3.p2.3.m3.6.6.2.2.2.2" xref="S3.p2.3.m3.6.6.2.2.2.3.cmml"><mrow id="S3.p2.3.m3.6.6.2.2.2.1.1" xref="S3.p2.3.m3.6.6.2.2.2.1.1.cmml"><msub id="S3.p2.3.m3.6.6.2.2.2.1.1.2" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2.cmml"><mi id="S3.p2.3.m3.6.6.2.2.2.1.1.2.2" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2.2.cmml">n</mi><mi id="S3.p2.3.m3.6.6.2.2.2.1.1.2.3" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2.3.cmml">j</mi></msub><mo id="S3.p2.3.m3.6.6.2.2.2.1.1.1" xref="S3.p2.3.m3.6.6.2.2.2.1.1.1.cmml">∈</mo><mtext class="ltx_mathvariant_monospace" id="S3.p2.3.m3.6.6.2.2.2.1.1.3" xref="S3.p2.3.m3.6.6.2.2.2.1.1.3a.cmml">str</mtext></mrow><mo id="S3.p2.3.m3.6.6.2.2.2.2.3" xref="S3.p2.3.m3.6.6.2.2.2.3a.cmml">,</mo><mrow id="S3.p2.3.m3.6.6.2.2.2.2.2" xref="S3.p2.3.m3.6.6.2.2.2.2.2.cmml"><mi id="S3.p2.3.m3.6.6.2.2.2.2.2.2" xref="S3.p2.3.m3.6.6.2.2.2.2.2.2.cmml">j</mi><mo id="S3.p2.3.m3.6.6.2.2.2.2.2.1" xref="S3.p2.3.m3.6.6.2.2.2.2.2.1.cmml">∈</mo><mrow id="S3.p2.3.m3.6.6.2.2.2.2.2.3.2" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml"><mo id="S3.p2.3.m3.6.6.2.2.2.2.2.3.2.1" stretchy="false" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml">[</mo><mn id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">1</mn><mo id="S3.p2.3.m3.6.6.2.2.2.2.2.3.2.2" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml">,</mo><mn id="S3.p2.3.m3.2.2" xref="S3.p2.3.m3.2.2.cmml">2</mn><mo id="S3.p2.3.m3.6.6.2.2.2.2.2.3.2.3" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.p2.3.m3.3.3" mathvariant="normal" xref="S3.p2.3.m3.3.3.cmml">…</mi><mo id="S3.p2.3.m3.6.6.2.2.2.2.2.3.2.4" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.p2.3.m3.4.4" xref="S3.p2.3.m3.4.4.cmml">k</mi><mo id="S3.p2.3.m3.6.6.2.2.2.2.2.3.2.5" stretchy="false" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.p2.3.m3.6.6.2.2.5" stretchy="false" xref="S3.p2.3.m3.6.6.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.6b"><apply id="S3.p2.3.m3.6.6.cmml" xref="S3.p2.3.m3.6.6"><eq id="S3.p2.3.m3.6.6.3.cmml" xref="S3.p2.3.m3.6.6.3"></eq><ci id="S3.p2.3.m3.6.6.4.cmml" xref="S3.p2.3.m3.6.6.4">𝑁</ci><apply id="S3.p2.3.m3.6.6.2.3.cmml" xref="S3.p2.3.m3.6.6.2.2"><csymbol cd="latexml" id="S3.p2.3.m3.6.6.2.3.1.cmml" xref="S3.p2.3.m3.6.6.2.2.3">conditional-set</csymbol><apply id="S3.p2.3.m3.5.5.1.1.1.cmml" xref="S3.p2.3.m3.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.5.5.1.1.1.1.cmml" xref="S3.p2.3.m3.5.5.1.1.1">subscript</csymbol><ci id="S3.p2.3.m3.5.5.1.1.1.2.cmml" xref="S3.p2.3.m3.5.5.1.1.1.2">𝑛</ci><ci id="S3.p2.3.m3.5.5.1.1.1.3.cmml" xref="S3.p2.3.m3.5.5.1.1.1.3">𝑗</ci></apply><apply id="S3.p2.3.m3.6.6.2.2.2.3.cmml" xref="S3.p2.3.m3.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.3.m3.6.6.2.2.2.3a.cmml" xref="S3.p2.3.m3.6.6.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.p2.3.m3.6.6.2.2.2.1.1.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1"><in id="S3.p2.3.m3.6.6.2.2.2.1.1.1.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.1"></in><apply id="S3.p2.3.m3.6.6.2.2.2.1.1.2.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.p2.3.m3.6.6.2.2.2.1.1.2.1.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2">subscript</csymbol><ci id="S3.p2.3.m3.6.6.2.2.2.1.1.2.2.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2.2">𝑛</ci><ci id="S3.p2.3.m3.6.6.2.2.2.1.1.2.3.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.2.3">𝑗</ci></apply><ci id="S3.p2.3.m3.6.6.2.2.2.1.1.3a.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.p2.3.m3.6.6.2.2.2.1.1.3.cmml" xref="S3.p2.3.m3.6.6.2.2.2.1.1.3">str</mtext></ci></apply><apply id="S3.p2.3.m3.6.6.2.2.2.2.2.cmml" xref="S3.p2.3.m3.6.6.2.2.2.2.2"><in id="S3.p2.3.m3.6.6.2.2.2.2.2.1.cmml" xref="S3.p2.3.m3.6.6.2.2.2.2.2.1"></in><ci id="S3.p2.3.m3.6.6.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.6.6.2.2.2.2.2.2">𝑗</ci><list id="S3.p2.3.m3.6.6.2.2.2.2.2.3.1.cmml" xref="S3.p2.3.m3.6.6.2.2.2.2.2.3.2"><cn id="S3.p2.3.m3.1.1.cmml" type="integer" xref="S3.p2.3.m3.1.1">1</cn><cn id="S3.p2.3.m3.2.2.cmml" type="integer" xref="S3.p2.3.m3.2.2">2</cn><ci id="S3.p2.3.m3.3.3.cmml" xref="S3.p2.3.m3.3.3">…</ci><ci id="S3.p2.3.m3.4.4.cmml" xref="S3.p2.3.m3.4.4">𝑘</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.6c">N=\{n_{j}|n_{j}\in\texttt{str},j\in[1,2,...,k]\}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.6d">italic_N = { italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ str , italic_j ∈ [ 1 , 2 , … , italic_k ] }</annotation></semantics></math>.
The multimodal user instruction can be just a text instruction <math alttext="d\in\texttt{str}" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">d</mi><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">∈</mo><mtext class="ltx_mathvariant_monospace" id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3a.cmml">str</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><in id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></in><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑑</ci><ci id="S3.p2.4.m4.1.1.3a.cmml" xref="S3.p2.4.m4.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">str</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">d\in\texttt{str}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_d ∈ str</annotation></semantics></math> (e.g., “Where can I find a ladder?”), or both text and image instructions <math alttext="I\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">I</mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml"><mi id="S3.p2.5.m5.1.1.3.2" xref="S3.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p2.5.m5.1.1.3.3" xref="S3.p2.5.m5.1.1.3.3.cmml"><mi id="S3.p2.5.m5.1.1.3.3.2" xref="S3.p2.5.m5.1.1.3.3.2.cmml">H</mi><mo id="S3.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.p2.5.m5.1.1.3.3.3" xref="S3.p2.5.m5.1.1.3.3.3.cmml">W</mi><mo id="S3.p2.5.m5.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.p2.5.m5.1.1.3.3.1.cmml">×</mo><mn id="S3.p2.5.m5.1.1.3.3.4" xref="S3.p2.5.m5.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><in id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></in><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝐼</ci><apply id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.3.1.cmml" xref="S3.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.p2.5.m5.1.1.3.2.cmml" xref="S3.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.p2.5.m5.1.1.3.3.cmml" xref="S3.p2.5.m5.1.1.3.3"><times id="S3.p2.5.m5.1.1.3.3.1.cmml" xref="S3.p2.5.m5.1.1.3.3.1"></times><ci id="S3.p2.5.m5.1.1.3.3.2.cmml" xref="S3.p2.5.m5.1.1.3.3.2">𝐻</ci><ci id="S3.p2.5.m5.1.1.3.3.3.cmml" xref="S3.p2.5.m5.1.1.3.3.3">𝑊</ci><cn id="S3.p2.5.m5.1.1.3.3.4.cmml" type="integer" xref="S3.p2.5.m5.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">I\in\mathbb{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">italic_I ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × 3 end_POSTSUPERSCRIPT</annotation></semantics></math> (e.g., “Where can I get something to clean this?” + The robot sees the user pointing to a dirty whiteboard).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.6">We aim to produce a navigation policy <math alttext="\pi(a|O,F,N,d,I)" class="ltx_Math" display="inline" id="S3.p3.1.m1.6"><semantics id="S3.p3.1.m1.6a"><mrow id="S3.p3.1.m1.6.6" xref="S3.p3.1.m1.6.6.cmml"><mi id="S3.p3.1.m1.6.6.3" xref="S3.p3.1.m1.6.6.3.cmml">π</mi><mo id="S3.p3.1.m1.6.6.2" xref="S3.p3.1.m1.6.6.2.cmml">⁢</mo><mrow id="S3.p3.1.m1.6.6.1.1" xref="S3.p3.1.m1.6.6.1.1.1.cmml"><mo id="S3.p3.1.m1.6.6.1.1.2" stretchy="false" xref="S3.p3.1.m1.6.6.1.1.1.cmml">(</mo><mrow id="S3.p3.1.m1.6.6.1.1.1" xref="S3.p3.1.m1.6.6.1.1.1.cmml"><mi id="S3.p3.1.m1.6.6.1.1.1.2" xref="S3.p3.1.m1.6.6.1.1.1.2.cmml">a</mi><mo fence="false" id="S3.p3.1.m1.6.6.1.1.1.1" xref="S3.p3.1.m1.6.6.1.1.1.1.cmml">|</mo><mrow id="S3.p3.1.m1.6.6.1.1.1.3.2" xref="S3.p3.1.m1.6.6.1.1.1.3.1.cmml"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">O</mi><mo id="S3.p3.1.m1.6.6.1.1.1.3.2.1" xref="S3.p3.1.m1.6.6.1.1.1.3.1.cmml">,</mo><mi id="S3.p3.1.m1.2.2" xref="S3.p3.1.m1.2.2.cmml">F</mi><mo id="S3.p3.1.m1.6.6.1.1.1.3.2.2" xref="S3.p3.1.m1.6.6.1.1.1.3.1.cmml">,</mo><mi id="S3.p3.1.m1.3.3" xref="S3.p3.1.m1.3.3.cmml">N</mi><mo id="S3.p3.1.m1.6.6.1.1.1.3.2.3" xref="S3.p3.1.m1.6.6.1.1.1.3.1.cmml">,</mo><mi id="S3.p3.1.m1.4.4" xref="S3.p3.1.m1.4.4.cmml">d</mi><mo id="S3.p3.1.m1.6.6.1.1.1.3.2.4" xref="S3.p3.1.m1.6.6.1.1.1.3.1.cmml">,</mo><mi id="S3.p3.1.m1.5.5" xref="S3.p3.1.m1.5.5.cmml">I</mi></mrow></mrow><mo id="S3.p3.1.m1.6.6.1.1.3" stretchy="false" xref="S3.p3.1.m1.6.6.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.6b"><apply id="S3.p3.1.m1.6.6.cmml" xref="S3.p3.1.m1.6.6"><times id="S3.p3.1.m1.6.6.2.cmml" xref="S3.p3.1.m1.6.6.2"></times><ci id="S3.p3.1.m1.6.6.3.cmml" xref="S3.p3.1.m1.6.6.3">𝜋</ci><apply id="S3.p3.1.m1.6.6.1.1.1.cmml" xref="S3.p3.1.m1.6.6.1.1"><csymbol cd="latexml" id="S3.p3.1.m1.6.6.1.1.1.1.cmml" xref="S3.p3.1.m1.6.6.1.1.1.1">conditional</csymbol><ci id="S3.p3.1.m1.6.6.1.1.1.2.cmml" xref="S3.p3.1.m1.6.6.1.1.1.2">𝑎</ci><list id="S3.p3.1.m1.6.6.1.1.1.3.1.cmml" xref="S3.p3.1.m1.6.6.1.1.1.3.2"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑂</ci><ci id="S3.p3.1.m1.2.2.cmml" xref="S3.p3.1.m1.2.2">𝐹</ci><ci id="S3.p3.1.m1.3.3.cmml" xref="S3.p3.1.m1.3.3">𝑁</ci><ci id="S3.p3.1.m1.4.4.cmml" xref="S3.p3.1.m1.4.4">𝑑</ci><ci id="S3.p3.1.m1.5.5.cmml" xref="S3.p3.1.m1.5.5">𝐼</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.6c">\pi(a|O,F,N,d,I)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.6d">italic_π ( italic_a | italic_O , italic_F , italic_N , italic_d , italic_I )</annotation></semantics></math>, where <math alttext="O\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">O</mi><mo id="S3.p3.2.m2.1.1.1" xref="S3.p3.2.m2.1.1.1.cmml">∈</mo><msup id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml"><mi id="S3.p3.2.m2.1.1.3.2" xref="S3.p3.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p3.2.m2.1.1.3.3" xref="S3.p3.2.m2.1.1.3.3.cmml"><mi id="S3.p3.2.m2.1.1.3.3.2" xref="S3.p3.2.m2.1.1.3.3.2.cmml">H</mi><mo id="S3.p3.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p3.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.p3.2.m2.1.1.3.3.3" xref="S3.p3.2.m2.1.1.3.3.3.cmml">W</mi><mo id="S3.p3.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.p3.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.p3.2.m2.1.1.3.3.4" xref="S3.p3.2.m2.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><in id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1"></in><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑂</ci><apply id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.3.1.cmml" xref="S3.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.p3.2.m2.1.1.3.2.cmml" xref="S3.p3.2.m2.1.1.3.2">ℝ</ci><apply id="S3.p3.2.m2.1.1.3.3.cmml" xref="S3.p3.2.m2.1.1.3.3"><times id="S3.p3.2.m2.1.1.3.3.1.cmml" xref="S3.p3.2.m2.1.1.3.3.1"></times><ci id="S3.p3.2.m2.1.1.3.3.2.cmml" xref="S3.p3.2.m2.1.1.3.3.2">𝐻</ci><ci id="S3.p3.2.m2.1.1.3.3.3.cmml" xref="S3.p3.2.m2.1.1.3.3.3">𝑊</ci><cn id="S3.p3.2.m2.1.1.3.3.4.cmml" type="integer" xref="S3.p3.2.m2.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">O\in\mathbb{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_O ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × 3 end_POSTSUPERSCRIPT</annotation></semantics></math> is the robot’s current camera observation.
The policy emits an embodiment-agnostic waypoint action <math alttext="a\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mrow id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">a</mi><mo id="S3.p3.3.m3.1.1.1" xref="S3.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml"><mi id="S3.p3.3.m3.1.1.3.2" xref="S3.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S3.p3.3.m3.1.1.3.3" xref="S3.p3.3.m3.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><in id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1.1"></in><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝑎</ci><apply id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.3.1.cmml" xref="S3.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.p3.3.m3.1.1.3.2.cmml" xref="S3.p3.3.m3.1.1.3.2">ℝ</ci><cn id="S3.p3.3.m3.1.1.3.3.cmml" type="integer" xref="S3.p3.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">a\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_a ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> representing longitudinal translation (<math alttext="\Delta x" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mrow id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mi id="S3.p3.4.m4.1.1.2" mathvariant="normal" xref="S3.p3.4.m4.1.1.2.cmml">Δ</mi><mo id="S3.p3.4.m4.1.1.1" xref="S3.p3.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><times id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1.1"></times><ci id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2">Δ</ci><ci id="S3.p3.4.m4.1.1.3.cmml" xref="S3.p3.4.m4.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">\Delta x</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">roman_Δ italic_x</annotation></semantics></math>), lateral translation (<math alttext="\Delta y" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mrow id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml"><mi id="S3.p3.5.m5.1.1.2" mathvariant="normal" xref="S3.p3.5.m5.1.1.2.cmml">Δ</mi><mo id="S3.p3.5.m5.1.1.1" xref="S3.p3.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.p3.5.m5.1.1.3" xref="S3.p3.5.m5.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><apply id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1"><times id="S3.p3.5.m5.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1"></times><ci id="S3.p3.5.m5.1.1.2.cmml" xref="S3.p3.5.m5.1.1.2">Δ</ci><ci id="S3.p3.5.m5.1.1.3.cmml" xref="S3.p3.5.m5.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">\Delta y</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">roman_Δ italic_y</annotation></semantics></math>), and rotation along the vertical axis (<math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S3.p3.6.m6.1"><semantics id="S3.p3.6.m6.1a"><mrow id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml"><mi id="S3.p3.6.m6.1.1.2" mathvariant="normal" xref="S3.p3.6.m6.1.1.2.cmml">Δ</mi><mo id="S3.p3.6.m6.1.1.1" xref="S3.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S3.p3.6.m6.1.1.3" xref="S3.p3.6.m6.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1"><times id="S3.p3.6.m6.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1"></times><ci id="S3.p3.6.m6.1.1.2.cmml" xref="S3.p3.6.m6.1.1.2">Δ</ci><ci id="S3.p3.6.m6.1.1.3.cmml" xref="S3.p3.6.m6.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.1d">roman_Δ italic_θ</annotation></semantics></math>), all in the robot-centric frame.
We assume that the robot has an embodiment-specific mechanism to execute waypoint actions.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_ERROR undefined" id="S4.1.1">\method</span>
</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<span class="ltx_ERROR undefined" id="S4.p1.8">\method</span>
<p class="ltx_p" id="S4.p1.7">is a hierarchical navigation policy (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S0.F1" title="Figure 1 ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a>) with online and offline components.
In the offline phase, a topological graph <math alttext="G" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_G</annotation></semantics></math> was generated from the demonstration tour (<math alttext="N,F" class="ltx_Math" display="inline" id="S4.p1.2.m2.2"><semantics id="S4.p1.2.m2.2a"><mrow id="S4.p1.2.m2.2.3.2" xref="S4.p1.2.m2.2.3.1.cmml"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">N</mi><mo id="S4.p1.2.m2.2.3.2.1" xref="S4.p1.2.m2.2.3.1.cmml">,</mo><mi id="S4.p1.2.m2.2.2" xref="S4.p1.2.m2.2.2.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.2b"><list id="S4.p1.2.m2.2.3.1.cmml" xref="S4.p1.2.m2.2.3.2"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝑁</ci><ci id="S4.p1.2.m2.2.2.cmml" xref="S4.p1.2.m2.2.2">𝐹</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.2c">N,F</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.2d">italic_N , italic_F</annotation></semantics></math>).
Online, the high-level policy takes the demonstration tour and the multimodal user instruction (<math alttext="d,I" class="ltx_Math" display="inline" id="S4.p1.3.m3.2"><semantics id="S4.p1.3.m3.2a"><mrow id="S4.p1.3.m3.2.3.2" xref="S4.p1.3.m3.2.3.1.cmml"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">d</mi><mo id="S4.p1.3.m3.2.3.2.1" xref="S4.p1.3.m3.2.3.1.cmml">,</mo><mi id="S4.p1.3.m3.2.2" xref="S4.p1.3.m3.2.2.cmml">I</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.2b"><list id="S4.p1.3.m3.2.3.1.cmml" xref="S4.p1.3.m3.2.3.2"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">𝑑</ci><ci id="S4.p1.3.m3.2.2.cmml" xref="S4.p1.3.m3.2.2">𝐼</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.2c">d,I</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.2d">italic_d , italic_I</annotation></semantics></math>) to find the navigation goal frame index <math alttext="g" class="ltx_Math" display="inline" id="S4.p1.4.m4.1"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">g</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_g</annotation></semantics></math>, which is an integer corresponding to a specific frame of the tour.
Next, the lower-level policy utilize the topological graph, the current camera observation (<math alttext="O" class="ltx_Math" display="inline" id="S4.p1.5.m5.1"><semantics id="S4.p1.5.m5.1a"><mi id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><ci id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">O</annotation><annotation encoding="application/x-llamapun" id="S4.p1.5.m5.1d">italic_O</annotation></semantics></math>) and <math alttext="g" class="ltx_Math" display="inline" id="S4.p1.6.m6.1"><semantics id="S4.p1.6.m6.1a"><mi id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><ci id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">g</annotation><annotation encoding="application/x-llamapun" id="S4.p1.6.m6.1d">italic_g</annotation></semantics></math> to produce a waypoint action (<math alttext="a" class="ltx_Math" display="inline" id="S4.p1.7.m7.1"><semantics id="S4.p1.7.m7.1a"><mi id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><ci id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.p1.7.m7.1d">italic_a</annotation></semantics></math>) for the robot to execute at each timestep.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx1">
<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle g=h(F,N,d,I)" class="ltx_Math" display="inline" id="S4.E1.m1.4"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.5" xref="S4.E1.m1.4.5.cmml"><mi id="S4.E1.m1.4.5.2" xref="S4.E1.m1.4.5.2.cmml">g</mi><mo id="S4.E1.m1.4.5.1" xref="S4.E1.m1.4.5.1.cmml">=</mo><mrow id="S4.E1.m1.4.5.3" xref="S4.E1.m1.4.5.3.cmml"><mi id="S4.E1.m1.4.5.3.2" xref="S4.E1.m1.4.5.3.2.cmml">h</mi><mo id="S4.E1.m1.4.5.3.1" xref="S4.E1.m1.4.5.3.1.cmml">⁢</mo><mrow id="S4.E1.m1.4.5.3.3.2" xref="S4.E1.m1.4.5.3.3.1.cmml"><mo id="S4.E1.m1.4.5.3.3.2.1" stretchy="false" xref="S4.E1.m1.4.5.3.3.1.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">F</mi><mo id="S4.E1.m1.4.5.3.3.2.2" xref="S4.E1.m1.4.5.3.3.1.cmml">,</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">N</mi><mo id="S4.E1.m1.4.5.3.3.2.3" xref="S4.E1.m1.4.5.3.3.1.cmml">,</mo><mi id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">d</mi><mo id="S4.E1.m1.4.5.3.3.2.4" xref="S4.E1.m1.4.5.3.3.1.cmml">,</mo><mi id="S4.E1.m1.4.4" xref="S4.E1.m1.4.4.cmml">I</mi><mo id="S4.E1.m1.4.5.3.3.2.5" stretchy="false" xref="S4.E1.m1.4.5.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.5.cmml" xref="S4.E1.m1.4.5"><eq id="S4.E1.m1.4.5.1.cmml" xref="S4.E1.m1.4.5.1"></eq><ci id="S4.E1.m1.4.5.2.cmml" xref="S4.E1.m1.4.5.2">𝑔</ci><apply id="S4.E1.m1.4.5.3.cmml" xref="S4.E1.m1.4.5.3"><times id="S4.E1.m1.4.5.3.1.cmml" xref="S4.E1.m1.4.5.3.1"></times><ci id="S4.E1.m1.4.5.3.2.cmml" xref="S4.E1.m1.4.5.3.2">ℎ</ci><vector id="S4.E1.m1.4.5.3.3.1.cmml" xref="S4.E1.m1.4.5.3.3.2"><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝐹</ci><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝑁</ci><ci id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">𝑑</ci><ci id="S4.E1.m1.4.4.cmml" xref="S4.E1.m1.4.4">𝐼</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">\displaystyle g=h(F,N,d,I)</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.4d">italic_g = italic_h ( italic_F , italic_N , italic_d , italic_I )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\pi(a|O,F,N,d,I)=l(a|G,O,g)" class="ltx_Math" display="inline" id="S4.E2.m1.10"><semantics id="S4.E2.m1.10a"><mrow id="S4.E2.m1.10.10" xref="S4.E2.m1.10.10.cmml"><mrow id="S4.E2.m1.9.9.1" xref="S4.E2.m1.9.9.1.cmml"><mi id="S4.E2.m1.9.9.1.3" xref="S4.E2.m1.9.9.1.3.cmml">π</mi><mo id="S4.E2.m1.9.9.1.2" xref="S4.E2.m1.9.9.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.9.9.1.1.1" xref="S4.E2.m1.9.9.1.1.1.1.cmml"><mo id="S4.E2.m1.9.9.1.1.1.2" stretchy="false" xref="S4.E2.m1.9.9.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.9.9.1.1.1.1" xref="S4.E2.m1.9.9.1.1.1.1.cmml"><mi id="S4.E2.m1.9.9.1.1.1.1.2" xref="S4.E2.m1.9.9.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.E2.m1.9.9.1.1.1.1.1" xref="S4.E2.m1.9.9.1.1.1.1.1.cmml">|</mo><mrow id="S4.E2.m1.9.9.1.1.1.1.3.2" xref="S4.E2.m1.9.9.1.1.1.1.3.1.cmml"><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">O</mi><mo id="S4.E2.m1.9.9.1.1.1.1.3.2.1" xref="S4.E2.m1.9.9.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml">F</mi><mo id="S4.E2.m1.9.9.1.1.1.1.3.2.2" xref="S4.E2.m1.9.9.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml">N</mi><mo id="S4.E2.m1.9.9.1.1.1.1.3.2.3" xref="S4.E2.m1.9.9.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml">d</mi><mo id="S4.E2.m1.9.9.1.1.1.1.3.2.4" xref="S4.E2.m1.9.9.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.5.5" xref="S4.E2.m1.5.5.cmml">I</mi></mrow></mrow><mo id="S4.E2.m1.9.9.1.1.1.3" stretchy="false" xref="S4.E2.m1.9.9.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.10.10.3" xref="S4.E2.m1.10.10.3.cmml">=</mo><mrow id="S4.E2.m1.10.10.2" xref="S4.E2.m1.10.10.2.cmml"><mi id="S4.E2.m1.10.10.2.3" xref="S4.E2.m1.10.10.2.3.cmml">l</mi><mo id="S4.E2.m1.10.10.2.2" xref="S4.E2.m1.10.10.2.2.cmml">⁢</mo><mrow id="S4.E2.m1.10.10.2.1.1" xref="S4.E2.m1.10.10.2.1.1.1.cmml"><mo id="S4.E2.m1.10.10.2.1.1.2" stretchy="false" xref="S4.E2.m1.10.10.2.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.10.10.2.1.1.1" xref="S4.E2.m1.10.10.2.1.1.1.cmml"><mi id="S4.E2.m1.10.10.2.1.1.1.2" xref="S4.E2.m1.10.10.2.1.1.1.2.cmml">a</mi><mo fence="false" id="S4.E2.m1.10.10.2.1.1.1.1" xref="S4.E2.m1.10.10.2.1.1.1.1.cmml">|</mo><mrow id="S4.E2.m1.10.10.2.1.1.1.3.2" xref="S4.E2.m1.10.10.2.1.1.1.3.1.cmml"><mi id="S4.E2.m1.6.6" xref="S4.E2.m1.6.6.cmml">G</mi><mo id="S4.E2.m1.10.10.2.1.1.1.3.2.1" xref="S4.E2.m1.10.10.2.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.7.7" xref="S4.E2.m1.7.7.cmml">O</mi><mo id="S4.E2.m1.10.10.2.1.1.1.3.2.2" xref="S4.E2.m1.10.10.2.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.8.8" xref="S4.E2.m1.8.8.cmml">g</mi></mrow></mrow><mo id="S4.E2.m1.10.10.2.1.1.3" stretchy="false" xref="S4.E2.m1.10.10.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.10b"><apply id="S4.E2.m1.10.10.cmml" xref="S4.E2.m1.10.10"><eq id="S4.E2.m1.10.10.3.cmml" xref="S4.E2.m1.10.10.3"></eq><apply id="S4.E2.m1.9.9.1.cmml" xref="S4.E2.m1.9.9.1"><times id="S4.E2.m1.9.9.1.2.cmml" xref="S4.E2.m1.9.9.1.2"></times><ci id="S4.E2.m1.9.9.1.3.cmml" xref="S4.E2.m1.9.9.1.3">𝜋</ci><apply id="S4.E2.m1.9.9.1.1.1.1.cmml" xref="S4.E2.m1.9.9.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.9.9.1.1.1.1.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.1">conditional</csymbol><ci id="S4.E2.m1.9.9.1.1.1.1.2.cmml" xref="S4.E2.m1.9.9.1.1.1.1.2">𝑎</ci><list id="S4.E2.m1.9.9.1.1.1.1.3.1.cmml" xref="S4.E2.m1.9.9.1.1.1.1.3.2"><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">𝑂</ci><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝐹</ci><ci id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3">𝑁</ci><ci id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4">𝑑</ci><ci id="S4.E2.m1.5.5.cmml" xref="S4.E2.m1.5.5">𝐼</ci></list></apply></apply><apply id="S4.E2.m1.10.10.2.cmml" xref="S4.E2.m1.10.10.2"><times id="S4.E2.m1.10.10.2.2.cmml" xref="S4.E2.m1.10.10.2.2"></times><ci id="S4.E2.m1.10.10.2.3.cmml" xref="S4.E2.m1.10.10.2.3">𝑙</ci><apply id="S4.E2.m1.10.10.2.1.1.1.cmml" xref="S4.E2.m1.10.10.2.1.1"><csymbol cd="latexml" id="S4.E2.m1.10.10.2.1.1.1.1.cmml" xref="S4.E2.m1.10.10.2.1.1.1.1">conditional</csymbol><ci id="S4.E2.m1.10.10.2.1.1.1.2.cmml" xref="S4.E2.m1.10.10.2.1.1.1.2">𝑎</ci><list id="S4.E2.m1.10.10.2.1.1.1.3.1.cmml" xref="S4.E2.m1.10.10.2.1.1.1.3.2"><ci id="S4.E2.m1.6.6.cmml" xref="S4.E2.m1.6.6">𝐺</ci><ci id="S4.E2.m1.7.7.cmml" xref="S4.E2.m1.7.7">𝑂</ci><ci id="S4.E2.m1.8.8.cmml" xref="S4.E2.m1.8.8">𝑔</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.10c">\displaystyle\pi(a|O,F,N,d,I)=l(a|G,O,g)</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.10d">italic_π ( italic_a | italic_O , italic_F , italic_N , italic_d , italic_I ) = italic_l ( italic_a | italic_G , italic_O , italic_g )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">where <math alttext="h" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">h</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">italic_h</annotation></semantics></math> and <math alttext="l" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_l</annotation></semantics></math> are the high and low-level policies.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Demonstration Tour and Offline Topological Graph Generation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<span class="ltx_ERROR undefined" id="S4.SS1.p1.1">\method</span>
<p class="ltx_p" id="S4.SS1.p1.2">utilizes a demonstration tour of the environment to solve MINT.
This tour can be given by a human user via teleoperation, or by simply recording a video on a <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.2.1">smartphone</span> while walking in the environment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<span class="ltx_ERROR undefined" id="S4.SS1.p2.7">\method</span>
<p class="ltx_p" id="S4.SS1.p2.6">then constructs a topological graph <math alttext="G=(V,E)" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.2"><semantics id="S4.SS1.p2.1.m1.2a"><mrow id="S4.SS1.p2.1.m1.2.3" xref="S4.SS1.p2.1.m1.2.3.cmml"><mi id="S4.SS1.p2.1.m1.2.3.2" xref="S4.SS1.p2.1.m1.2.3.2.cmml">G</mi><mo id="S4.SS1.p2.1.m1.2.3.1" xref="S4.SS1.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS1.p2.1.m1.2.3.3.2" xref="S4.SS1.p2.1.m1.2.3.3.1.cmml"><mo id="S4.SS1.p2.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS1.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">V</mi><mo id="S4.SS1.p2.1.m1.2.3.3.2.2" xref="S4.SS1.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS1.p2.1.m1.2.2" xref="S4.SS1.p2.1.m1.2.2.cmml">E</mi><mo id="S4.SS1.p2.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS1.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.2b"><apply id="S4.SS1.p2.1.m1.2.3.cmml" xref="S4.SS1.p2.1.m1.2.3"><eq id="S4.SS1.p2.1.m1.2.3.1.cmml" xref="S4.SS1.p2.1.m1.2.3.1"></eq><ci id="S4.SS1.p2.1.m1.2.3.2.cmml" xref="S4.SS1.p2.1.m1.2.3.2">𝐺</ci><interval closure="open" id="S4.SS1.p2.1.m1.2.3.3.1.cmml" xref="S4.SS1.p2.1.m1.2.3.3.2"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑉</ci><ci id="S4.SS1.p2.1.m1.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2">𝐸</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.2c">G=(V,E)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.2d">italic_G = ( italic_V , italic_E )</annotation></semantics></math> offline, where each vertex <math alttext="v_{i}\in V" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><msub id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2.2" xref="S4.SS1.p2.2.m2.1.1.2.2.cmml">v</mi><mi id="S4.SS1.p2.2.m2.1.1.2.3" xref="S4.SS1.p2.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">∈</mo><mi id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><in id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></in><apply id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2.2">𝑣</ci><ci id="S4.SS1.p2.2.m2.1.1.2.3.cmml" xref="S4.SS1.p2.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">v_{i}\in V</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_V</annotation></semantics></math> corresponds to the frame <math alttext="f_{i}" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">f</mi><mi id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑓</ci><ci id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">f_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from the demonstration tour video (<math alttext="F" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">F</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_F</annotation></semantics></math>, <math alttext="N" class="ltx_Math" display="inline" id="S4.SS1.p2.5.m5.1"><semantics id="S4.SS1.p2.5.m5.1a"><mi id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.1d">italic_N</annotation></semantics></math>).
We use COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib52" title="">52</a>]</cite>, an off-the-shelf structure-from-motion pipeline to determine the approximate 6-Degree-of-Freedom camera pose for each frame and store it in the vertex (see Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS1" title="7.1 Structure-from-Motion and Hierarchical Localization ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.1</span></a> for details).
Next, a directed edge is added to <math alttext="G" class="ltx_Math" display="inline" id="S4.SS1.p2.6.m6.1"><semantics id="S4.SS1.p2.6.m6.1a"><mi id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><ci id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.1d">italic_G</annotation></semantics></math> if the target vertex is “in front of” the source vertex (less than 90 degrees away from source vertex’s pose) and is within 2m.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Compared to traditional navigation pipelines (e.g., map the environment, identify traversable areas and then construct a PRM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib53" title="">53</a>]</cite>), the topological graph approach significantly simpler as it captures the general connectivity of the environment based on the tour trajectory.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>High-Level Goal Finding with Long-Context Multimodal VLMs</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">During online execution, the high-level policy leverages the common sense reasoning ability of VLMs to identify a navigation goal from the demonstration tour that satisfies a wide range of multimodal, colloquial and often ambiguous user instructions.
To this end, we prepare a prompt <math alttext="P(F,N,d,I)" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.4"><semantics id="S4.SS2.p1.1.m1.4a"><mrow id="S4.SS2.p1.1.m1.4.5" xref="S4.SS2.p1.1.m1.4.5.cmml"><mi id="S4.SS2.p1.1.m1.4.5.2" xref="S4.SS2.p1.1.m1.4.5.2.cmml">P</mi><mo id="S4.SS2.p1.1.m1.4.5.1" xref="S4.SS2.p1.1.m1.4.5.1.cmml">⁢</mo><mrow id="S4.SS2.p1.1.m1.4.5.3.2" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml"><mo id="S4.SS2.p1.1.m1.4.5.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">(</mo><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">F</mi><mo id="S4.SS2.p1.1.m1.4.5.3.2.2" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">,</mo><mi id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">N</mi><mo id="S4.SS2.p1.1.m1.4.5.3.2.3" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">,</mo><mi id="S4.SS2.p1.1.m1.3.3" xref="S4.SS2.p1.1.m1.3.3.cmml">d</mi><mo id="S4.SS2.p1.1.m1.4.5.3.2.4" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">,</mo><mi id="S4.SS2.p1.1.m1.4.4" xref="S4.SS2.p1.1.m1.4.4.cmml">I</mi><mo id="S4.SS2.p1.1.m1.4.5.3.2.5" stretchy="false" xref="S4.SS2.p1.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.4b"><apply id="S4.SS2.p1.1.m1.4.5.cmml" xref="S4.SS2.p1.1.m1.4.5"><times id="S4.SS2.p1.1.m1.4.5.1.cmml" xref="S4.SS2.p1.1.m1.4.5.1"></times><ci id="S4.SS2.p1.1.m1.4.5.2.cmml" xref="S4.SS2.p1.1.m1.4.5.2">𝑃</ci><vector id="S4.SS2.p1.1.m1.4.5.3.1.cmml" xref="S4.SS2.p1.1.m1.4.5.3.2"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝐹</ci><ci id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2">𝑁</ci><ci id="S4.SS2.p1.1.m1.3.3.cmml" xref="S4.SS2.p1.1.m1.3.3">𝑑</ci><ci id="S4.SS2.p1.1.m1.4.4.cmml" xref="S4.SS2.p1.1.m1.4.4">𝐼</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.4c">P(F,N,d,I)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.4d">italic_P ( italic_F , italic_N , italic_d , italic_I )</annotation></semantics></math> consisting of interleaving text and images.
A concrete example of <math alttext="P" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_P</annotation></semantics></math> for the multimodal user instruction “Where should I return this?” in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T1" title="Table 1 ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a> is shown below:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="S4.SS2.p2.1" style="background-color:#F5F5F5;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,WW91IGFyZSBhIHJvYm90IG9wZXJhdGluZyBpbiBhIGJ1aWxkaW5nIGFuZCB5b3VyIHRhc2sgaXMgdG8gcmVzcG9uZCB0byB0aGUgdXNlcgpjb21tYW5kIGFib3V0IGdvaW5nIHRvIGEgc3BlY2lmaWMgbG9jYXRpb24gYnkgZmluZGluZyB0aGUgY2xvc2VzdCBmcmFtZSBpbiB0aGUKdG91ciB2aWRlbyB0byBuYXZpZ2F0ZSB0by4KVGhlc2UgZnJhbWVzIGFyZSBmcm9tIHRoZSB0b3VyIG9mIHRoZSBidWlsZGluZyBsYXN0IHllYXIuCltGcmFtZSAxIEltYWdlICRmXzEkXQpGcmFtZSAxLiBbRnJhbWUgbmFycmF0aXZlICRuXzEkXQouLi4KW0ZyYW1lIGsgSW1hZ2UgJGZfayRdCkZyYW1lIGsuIFtGcmFtZSBuYXJyYXRpdmUgJG5fayRdClRoaXMgaW1hZ2UgaXMgd2hhdCB5b3Ugc2VlIG5vdy4gWW91IG1heSBvciBtYXkgbm90IHNlZSB0aGUgdXNlciBpbiB0aGlzIGltYWdlLgpbSW1hZ2UgSW5zdHJ1Y3Rpb24gSV0KVGhlIHVzZXIgc2F5czogV2hlcmUgc2hvdWxkIEkgcmV0dXJuIHRoaXM/CkhvdyB3b3VsZCB5b3UgcmVzcG9uZD8gQ2FuIHlvdSBmaW5kIHRoZSBjbG9zZXN0IGZyYW1lPw==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.5">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.7">robot</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.9">operating</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.11">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.13">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.15">building</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.17">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.19">your</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.21">task</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.23">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.25">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.27">respond</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.29">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.31">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.33">user</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.1">command</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.3">about</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.5">going</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.9">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.11">specific</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.13">location</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.15">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.17">finding</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.19">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.21">closest</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.23">frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.25">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.27">the</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.1">tour</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.3">video</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.5">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.7">navigate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.9">to</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.10">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.1">These</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3">frames</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.5">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.7">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.11">tour</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.15">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.17">building</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.19">last</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.21">year</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.22">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx5.1">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.2">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.4">1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.6">Image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.8">$f_1$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.9">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.1">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3">1.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.5">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.6">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.8">narrative</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.10">$n_1$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.11">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx7.1">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx8.1">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.2">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.4">k</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.6">Image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.8">$f_k$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.9">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.1">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.3">k</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.6">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.7">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.9">narrative</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.11">$n_k$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.12">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.1">This</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.3">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.5">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.7">what</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.9">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.11">see</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.13">now</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.14">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.16">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.18">may</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.20">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.22">may</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.24">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.26">see</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.28">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.30">user</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.32">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.34">this</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.36">image</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.37">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx11.1">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.2">Image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.4">Instruction</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.6">I</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.7">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.1">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.3">user</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.5">says</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.6">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.8">Where</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.10">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.12">I</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.14">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.16">this</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.17">?</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.1">How</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.3">would</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.5">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.7">respond</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.8">?</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.10">Can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.12">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.14">find</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.16">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.18">closest</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.20">frame</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.21">?</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The VLM returns an integer goal frame index <math alttext="g" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_g</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Low-level Goal Reaching using Topological Graphs</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Once the goal frame index <math alttext="g" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_g</annotation></semantics></math> is identified by the high-level policy, the low-level policy (Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#alg1" title="Algorithm 1 ‣ 4.3 Low-level Goal Reaching using Topological Graphs ‣ 4 \method ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a>) takes over and produces a waypoint action at every timestep (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S4.E1" title="In 4 \method ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.3.1.1">Algorithm 1</span> </span> <span class="ltx_text ltx_font_bold" id="alg1.4.2">Low-level Goal Reaching Policy</span></figcaption>
<div class="ltx_listing ltx_listing" id="alg1.5">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="alg1.l1.1">Input:</span> goal frame index <math alttext="g" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_g</annotation></semantics></math>, offline-constructed topological graph <math alttext="G" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">G</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_G</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><span class="ltx_text ltx_font_bold" id="alg1.l3.1">while</span> timestep <math alttext="\leq" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mo id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><leq id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">≤</annotation></semantics></math> maximum steps <span class="ltx_text ltx_font_bold" id="alg1.l3.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>     Get new camera observation image <math alttext="O" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">O</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">italic_O</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>     Get start vertex <math alttext="v_{s}" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><msub id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">v</mi><mi id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1">subscript</csymbol><ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">𝑣</ci><ci id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">v_{s}</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and robot pose <math alttext="T" class="ltx_Math" display="inline" id="alg1.l5.m2.1"><semantics id="alg1.l5.m2.1a"><mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m2.1d">italic_T</annotation></semantics></math> by localizing <math alttext="O" class="ltx_Math" display="inline" id="alg1.l5.m3.1"><semantics id="alg1.l5.m3.1a"><mi id="alg1.l5.m3.1.1" xref="alg1.l5.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m3.1b"><ci id="alg1.l5.m3.1.1.cmml" xref="alg1.l5.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m3.1c">O</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m3.1d">italic_O</annotation></semantics></math> in <math alttext="G" class="ltx_Math" display="inline" id="alg1.l5.m4.1"><semantics id="alg1.l5.m4.1a"><mi id="alg1.l5.m4.1.1" xref="alg1.l5.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m4.1b"><ci id="alg1.l5.m4.1.1.cmml" xref="alg1.l5.m4.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m4.1c">G</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m4.1d">italic_G</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l6.1">if</span> <math alttext="v_{s}==v_{g}" class="ltx_math_unparsed" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1b"><msub id="alg1.l6.m1.1.1"><mi id="alg1.l6.m1.1.1.2">v</mi><mi id="alg1.l6.m1.1.1.3">s</mi></msub><mo id="alg1.l6.m1.1.2" rspace="0em">=</mo><mo id="alg1.l6.m1.1.3" lspace="0em">=</mo><msub id="alg1.l6.m1.1.4"><mi id="alg1.l6.m1.1.4.2">v</mi><mi id="alg1.l6.m1.1.4.3">g</mi></msub></mrow><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">v_{s}==v_{g}</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = = italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l6.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>         Navigation goal reached, break

</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l8.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l8.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>     Compute <math alttext="S=[v_{s},v_{1},...,v_{g}]" class="ltx_Math" display="inline" id="alg1.l9.m1.4"><semantics id="alg1.l9.m1.4a"><mrow id="alg1.l9.m1.4.4" xref="alg1.l9.m1.4.4.cmml"><mi id="alg1.l9.m1.4.4.5" xref="alg1.l9.m1.4.4.5.cmml">S</mi><mo id="alg1.l9.m1.4.4.4" xref="alg1.l9.m1.4.4.4.cmml">=</mo><mrow id="alg1.l9.m1.4.4.3.3" xref="alg1.l9.m1.4.4.3.4.cmml"><mo id="alg1.l9.m1.4.4.3.3.4" stretchy="false" xref="alg1.l9.m1.4.4.3.4.cmml">[</mo><msub id="alg1.l9.m1.2.2.1.1.1" xref="alg1.l9.m1.2.2.1.1.1.cmml"><mi id="alg1.l9.m1.2.2.1.1.1.2" xref="alg1.l9.m1.2.2.1.1.1.2.cmml">v</mi><mi id="alg1.l9.m1.2.2.1.1.1.3" xref="alg1.l9.m1.2.2.1.1.1.3.cmml">s</mi></msub><mo id="alg1.l9.m1.4.4.3.3.5" xref="alg1.l9.m1.4.4.3.4.cmml">,</mo><msub id="alg1.l9.m1.3.3.2.2.2" xref="alg1.l9.m1.3.3.2.2.2.cmml"><mi id="alg1.l9.m1.3.3.2.2.2.2" xref="alg1.l9.m1.3.3.2.2.2.2.cmml">v</mi><mn id="alg1.l9.m1.3.3.2.2.2.3" xref="alg1.l9.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="alg1.l9.m1.4.4.3.3.6" xref="alg1.l9.m1.4.4.3.4.cmml">,</mo><mi id="alg1.l9.m1.1.1" mathvariant="normal" xref="alg1.l9.m1.1.1.cmml">…</mi><mo id="alg1.l9.m1.4.4.3.3.7" xref="alg1.l9.m1.4.4.3.4.cmml">,</mo><msub id="alg1.l9.m1.4.4.3.3.3" xref="alg1.l9.m1.4.4.3.3.3.cmml"><mi id="alg1.l9.m1.4.4.3.3.3.2" xref="alg1.l9.m1.4.4.3.3.3.2.cmml">v</mi><mi id="alg1.l9.m1.4.4.3.3.3.3" xref="alg1.l9.m1.4.4.3.3.3.3.cmml">g</mi></msub><mo id="alg1.l9.m1.4.4.3.3.8" stretchy="false" xref="alg1.l9.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.4b"><apply id="alg1.l9.m1.4.4.cmml" xref="alg1.l9.m1.4.4"><eq id="alg1.l9.m1.4.4.4.cmml" xref="alg1.l9.m1.4.4.4"></eq><ci id="alg1.l9.m1.4.4.5.cmml" xref="alg1.l9.m1.4.4.5">𝑆</ci><list id="alg1.l9.m1.4.4.3.4.cmml" xref="alg1.l9.m1.4.4.3.3"><apply id="alg1.l9.m1.2.2.1.1.1.cmml" xref="alg1.l9.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l9.m1.2.2.1.1.1.1.cmml" xref="alg1.l9.m1.2.2.1.1.1">subscript</csymbol><ci id="alg1.l9.m1.2.2.1.1.1.2.cmml" xref="alg1.l9.m1.2.2.1.1.1.2">𝑣</ci><ci id="alg1.l9.m1.2.2.1.1.1.3.cmml" xref="alg1.l9.m1.2.2.1.1.1.3">𝑠</ci></apply><apply id="alg1.l9.m1.3.3.2.2.2.cmml" xref="alg1.l9.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="alg1.l9.m1.3.3.2.2.2.1.cmml" xref="alg1.l9.m1.3.3.2.2.2">subscript</csymbol><ci id="alg1.l9.m1.3.3.2.2.2.2.cmml" xref="alg1.l9.m1.3.3.2.2.2.2">𝑣</ci><cn id="alg1.l9.m1.3.3.2.2.2.3.cmml" type="integer" xref="alg1.l9.m1.3.3.2.2.2.3">1</cn></apply><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">…</ci><apply id="alg1.l9.m1.4.4.3.3.3.cmml" xref="alg1.l9.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="alg1.l9.m1.4.4.3.3.3.1.cmml" xref="alg1.l9.m1.4.4.3.3.3">subscript</csymbol><ci id="alg1.l9.m1.4.4.3.3.3.2.cmml" xref="alg1.l9.m1.4.4.3.3.3.2">𝑣</ci><ci id="alg1.l9.m1.4.4.3.3.3.3.cmml" xref="alg1.l9.m1.4.4.3.3.3.3">𝑔</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.4c">S=[v_{s},v_{1},...,v_{g}]</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.4d">italic_S = [ italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ]</annotation></semantics></math>, the shortest path between <math alttext="v_{s}" class="ltx_Math" display="inline" id="alg1.l9.m2.1"><semantics id="alg1.l9.m2.1a"><msub id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml"><mi id="alg1.l9.m2.1.1.2" xref="alg1.l9.m2.1.1.2.cmml">v</mi><mi id="alg1.l9.m2.1.1.3" xref="alg1.l9.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><apply id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.1.cmml" xref="alg1.l9.m2.1.1">subscript</csymbol><ci id="alg1.l9.m2.1.1.2.cmml" xref="alg1.l9.m2.1.1.2">𝑣</ci><ci id="alg1.l9.m2.1.1.3.cmml" xref="alg1.l9.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">v_{s}</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m2.1d">italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="v_{g}" class="ltx_Math" display="inline" id="alg1.l9.m3.1"><semantics id="alg1.l9.m3.1a"><msub id="alg1.l9.m3.1.1" xref="alg1.l9.m3.1.1.cmml"><mi id="alg1.l9.m3.1.1.2" xref="alg1.l9.m3.1.1.2.cmml">v</mi><mi id="alg1.l9.m3.1.1.3" xref="alg1.l9.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l9.m3.1b"><apply id="alg1.l9.m3.1.1.cmml" xref="alg1.l9.m3.1.1"><csymbol cd="ambiguous" id="alg1.l9.m3.1.1.1.cmml" xref="alg1.l9.m3.1.1">subscript</csymbol><ci id="alg1.l9.m3.1.1.2.cmml" xref="alg1.l9.m3.1.1.2">𝑣</ci><ci id="alg1.l9.m3.1.1.3.cmml" xref="alg1.l9.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m3.1c">v_{g}</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m3.1d">italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>     Compute waypoint action <math alttext="a" class="ltx_Math" display="inline" id="alg1.l10.m1.1"><semantics id="alg1.l10.m1.1a"><mi id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.1d">italic_a</annotation></semantics></math> from the relative pose between <math alttext="T" class="ltx_Math" display="inline" id="alg1.l10.m2.1"><semantics id="alg1.l10.m2.1a"><mi id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><ci id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m2.1d">italic_T</annotation></semantics></math> and <math alttext="v_{1}" class="ltx_Math" display="inline" id="alg1.l10.m3.1"><semantics id="alg1.l10.m3.1a"><msub id="alg1.l10.m3.1.1" xref="alg1.l10.m3.1.1.cmml"><mi id="alg1.l10.m3.1.1.2" xref="alg1.l10.m3.1.1.2.cmml">v</mi><mn id="alg1.l10.m3.1.1.3" xref="alg1.l10.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l10.m3.1b"><apply id="alg1.l10.m3.1.1.cmml" xref="alg1.l10.m3.1.1"><csymbol cd="ambiguous" id="alg1.l10.m3.1.1.1.cmml" xref="alg1.l10.m3.1.1">subscript</csymbol><ci id="alg1.l10.m3.1.1.2.cmml" xref="alg1.l10.m3.1.1.2">𝑣</ci><cn id="alg1.l10.m3.1.1.3.cmml" type="integer" xref="alg1.l10.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m3.1c">v_{1}</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m3.1d">italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>     Execute <math alttext="a" class="ltx_Math" display="inline" id="alg1.l11.m1.1"><semantics id="alg1.l11.m1.1a"><mi id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.1d">italic_a</annotation></semantics></math> on robot

</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span><span class="ltx_text ltx_font_bold" id="alg1.l12.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l12.2">while</span>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.15">At every timestep, we use a real-time hierarchical visual localization system (described briefly below, please see Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS1" title="7.1 Structure-from-Motion and Hierarchical Localization ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.1</span></a> for more details) to estimate the pose of the robot <math alttext="T" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_T</annotation></semantics></math> and the closest start vertex <math alttext="v_{s}\in G" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><msub id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2.2" xref="S4.SS3.p2.2.m2.1.1.2.2.cmml">v</mi><mi id="S4.SS3.p2.2.m2.1.1.2.3" xref="S4.SS3.p2.2.m2.1.1.2.3.cmml">s</mi></msub><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">∈</mo><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><in id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></in><apply id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.2.1.cmml" xref="S4.SS3.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2.2">𝑣</ci><ci id="S4.SS3.p2.2.m2.1.1.2.3.cmml" xref="S4.SS3.p2.2.m2.1.1.2.3">𝑠</ci></apply><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">v_{s}\in G</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∈ italic_G</annotation></semantics></math> (line 5) using the current camera observation <math alttext="O" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mi id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><ci id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">O</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">italic_O</annotation></semantics></math>.
This localization system finds k-nearest candidate frames in <math alttext="G" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mi id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><ci id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">italic_G</annotation></semantics></math> w.r.t a global descriptor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib54" title="">54</a>]</cite>, and then computes <math alttext="T" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mi id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><ci id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">italic_T</annotation></semantics></math> through PnP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib55" title="">55</a>]</cite>.
Next, the shortest path <math alttext="S" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><mi id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><ci id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">S</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.6.m6.1d">italic_S</annotation></semantics></math> on the topological graph between <math alttext="v_{s}" class="ltx_Math" display="inline" id="S4.SS3.p2.7.m7.1"><semantics id="S4.SS3.p2.7.m7.1a"><msub id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml"><mi id="S4.SS3.p2.7.m7.1.1.2" xref="S4.SS3.p2.7.m7.1.1.2.cmml">v</mi><mi id="S4.SS3.p2.7.m7.1.1.3" xref="S4.SS3.p2.7.m7.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.7.m7.1.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS3.p2.7.m7.1.1.2.cmml" xref="S4.SS3.p2.7.m7.1.1.2">𝑣</ci><ci id="S4.SS3.p2.7.m7.1.1.3.cmml" xref="S4.SS3.p2.7.m7.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">v_{s}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.7.m7.1d">italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and the goal vertex <math alttext="v_{g}" class="ltx_Math" display="inline" id="S4.SS3.p2.8.m8.1"><semantics id="S4.SS3.p2.8.m8.1a"><msub id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml"><mi id="S4.SS3.p2.8.m8.1.1.2" xref="S4.SS3.p2.8.m8.1.1.2.cmml">v</mi><mi id="S4.SS3.p2.8.m8.1.1.3" xref="S4.SS3.p2.8.m8.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><apply id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.8.m8.1.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS3.p2.8.m8.1.1.2.cmml" xref="S4.SS3.p2.8.m8.1.1.2">𝑣</ci><ci id="S4.SS3.p2.8.m8.1.1.3.cmml" xref="S4.SS3.p2.8.m8.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">v_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.8.m8.1d">italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> (the vertex corresponding to <math alttext="g" class="ltx_Math" display="inline" id="S4.SS3.p2.9.m9.1"><semantics id="S4.SS3.p2.9.m9.1a"><mi id="S4.SS3.p2.9.m9.1.1" xref="S4.SS3.p2.9.m9.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m9.1b"><ci id="S4.SS3.p2.9.m9.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m9.1c">g</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.9.m9.1d">italic_g</annotation></semantics></math>) is identified by Dijkstra’s algorithm (line 9).
Finally, the low-level policy returns a waypoint action which is simply the <math alttext="\Delta x" class="ltx_Math" display="inline" id="S4.SS3.p2.10.m10.1"><semantics id="S4.SS3.p2.10.m10.1a"><mrow id="S4.SS3.p2.10.m10.1.1" xref="S4.SS3.p2.10.m10.1.1.cmml"><mi id="S4.SS3.p2.10.m10.1.1.2" mathvariant="normal" xref="S4.SS3.p2.10.m10.1.1.2.cmml">Δ</mi><mo id="S4.SS3.p2.10.m10.1.1.1" xref="S4.SS3.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.10.m10.1.1.3" xref="S4.SS3.p2.10.m10.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.10.m10.1b"><apply id="S4.SS3.p2.10.m10.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1"><times id="S4.SS3.p2.10.m10.1.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1.1"></times><ci id="S4.SS3.p2.10.m10.1.1.2.cmml" xref="S4.SS3.p2.10.m10.1.1.2">Δ</ci><ci id="S4.SS3.p2.10.m10.1.1.3.cmml" xref="S4.SS3.p2.10.m10.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.10.m10.1c">\Delta x</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.10.m10.1d">roman_Δ italic_x</annotation></semantics></math>, <math alttext="\Delta y" class="ltx_Math" display="inline" id="S4.SS3.p2.11.m11.1"><semantics id="S4.SS3.p2.11.m11.1a"><mrow id="S4.SS3.p2.11.m11.1.1" xref="S4.SS3.p2.11.m11.1.1.cmml"><mi id="S4.SS3.p2.11.m11.1.1.2" mathvariant="normal" xref="S4.SS3.p2.11.m11.1.1.2.cmml">Δ</mi><mo id="S4.SS3.p2.11.m11.1.1.1" xref="S4.SS3.p2.11.m11.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.11.m11.1.1.3" xref="S4.SS3.p2.11.m11.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.11.m11.1b"><apply id="S4.SS3.p2.11.m11.1.1.cmml" xref="S4.SS3.p2.11.m11.1.1"><times id="S4.SS3.p2.11.m11.1.1.1.cmml" xref="S4.SS3.p2.11.m11.1.1.1"></times><ci id="S4.SS3.p2.11.m11.1.1.2.cmml" xref="S4.SS3.p2.11.m11.1.1.2">Δ</ci><ci id="S4.SS3.p2.11.m11.1.1.3.cmml" xref="S4.SS3.p2.11.m11.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.11.m11.1c">\Delta y</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.11.m11.1d">roman_Δ italic_y</annotation></semantics></math>, <math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S4.SS3.p2.12.m12.1"><semantics id="S4.SS3.p2.12.m12.1a"><mrow id="S4.SS3.p2.12.m12.1.1" xref="S4.SS3.p2.12.m12.1.1.cmml"><mi id="S4.SS3.p2.12.m12.1.1.2" mathvariant="normal" xref="S4.SS3.p2.12.m12.1.1.2.cmml">Δ</mi><mo id="S4.SS3.p2.12.m12.1.1.1" xref="S4.SS3.p2.12.m12.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.12.m12.1.1.3" xref="S4.SS3.p2.12.m12.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.12.m12.1b"><apply id="S4.SS3.p2.12.m12.1.1.cmml" xref="S4.SS3.p2.12.m12.1.1"><times id="S4.SS3.p2.12.m12.1.1.1.cmml" xref="S4.SS3.p2.12.m12.1.1.1"></times><ci id="S4.SS3.p2.12.m12.1.1.2.cmml" xref="S4.SS3.p2.12.m12.1.1.2">Δ</ci><ci id="S4.SS3.p2.12.m12.1.1.3.cmml" xref="S4.SS3.p2.12.m12.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.12.m12.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.12.m12.1d">roman_Δ italic_θ</annotation></semantics></math> of the next vertex <math alttext="v_{1}" class="ltx_Math" display="inline" id="S4.SS3.p2.13.m13.1"><semantics id="S4.SS3.p2.13.m13.1a"><msub id="S4.SS3.p2.13.m13.1.1" xref="S4.SS3.p2.13.m13.1.1.cmml"><mi id="S4.SS3.p2.13.m13.1.1.2" xref="S4.SS3.p2.13.m13.1.1.2.cmml">v</mi><mn id="S4.SS3.p2.13.m13.1.1.3" xref="S4.SS3.p2.13.m13.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.13.m13.1b"><apply id="S4.SS3.p2.13.m13.1.1.cmml" xref="S4.SS3.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.13.m13.1.1.1.cmml" xref="S4.SS3.p2.13.m13.1.1">subscript</csymbol><ci id="S4.SS3.p2.13.m13.1.1.2.cmml" xref="S4.SS3.p2.13.m13.1.1.2">𝑣</ci><cn id="S4.SS3.p2.13.m13.1.1.3.cmml" type="integer" xref="S4.SS3.p2.13.m13.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.13.m13.1c">v_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.13.m13.1d">italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="S" class="ltx_Math" display="inline" id="S4.SS3.p2.14.m14.1"><semantics id="S4.SS3.p2.14.m14.1a"><mi id="S4.SS3.p2.14.m14.1.1" xref="S4.SS3.p2.14.m14.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.14.m14.1b"><ci id="S4.SS3.p2.14.m14.1.1.cmml" xref="S4.SS3.p2.14.m14.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.14.m14.1c">S</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.14.m14.1d">italic_S</annotation></semantics></math> relative to <math alttext="T" class="ltx_Math" display="inline" id="S4.SS3.p2.15.m15.1"><semantics id="S4.SS3.p2.15.m15.1a"><mi id="S4.SS3.p2.15.m15.1.1" xref="S4.SS3.p2.15.m15.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.15.m15.1b"><ci id="S4.SS3.p2.15.m15.1.1.cmml" xref="S4.SS3.p2.15.m15.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.15.m15.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.15.m15.1d">italic_T</annotation></semantics></math> (line 10).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To demonstrate the performance of <span class="ltx_ERROR undefined" id="S5.p1.1.1">\method</span> and gain further insights into key designs, we design experiments to answer the following research questions (RQs):</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.1.1.1">RQ1:</span></span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Does <span class="ltx_ERROR undefined" id="S5.I1.i1.p1.1.1">\method</span> perform well in MINT in the real world?
</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.1.1.1">RQ2:</span></span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Does <span class="ltx_ERROR undefined" id="S5.I1.i2.p1.1.1">\method</span> outperform alternatives thanks to the use of long-context VLM?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.1.1.1">RQ3:</span></span>
<div class="ltx_para ltx_noindent" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Is the topological graph necessary? Can VLMs produce actions directly?</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T1.14">
<tr class="ltx_tr" id="S5.T1.14.15">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T1.14.15.1"><span class="ltx_text ltx_font_bold" id="S5.T1.14.15.1.1">Reasoning-Free (RF)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T1.14.15.2"><span class="ltx_text ltx_font_bold" id="S5.T1.14.15.2.1">Small Object (SO)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1">Text instr. (<math alttext="d" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mi id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">italic_d</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.2.2.2">Goal frame (<math alttext="g" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.m1.1a"><mi id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m1.1d">italic_g</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.3">Text instr. (<math alttext="d" class="ltx_Math" display="inline" id="S5.T1.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.m1.1a"><mi id="S5.T1.3.3.3.m1.1.1" xref="S5.T1.3.3.3.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.m1.1d">italic_d</annotation></semantics></math>)</td>
<td class="ltx_td ltx_border_t" id="S5.T1.4.4.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.4.4">Goal frame (<math alttext="g" class="ltx_Math" display="inline" id="S5.T1.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.m1.1a"><mi id="S5.T1.4.4.4.m1.1.1" xref="S5.T1.4.4.4.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.m1.1d">italic_g</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T1.6.6">
<td class="ltx_td ltx_border_t" id="S5.T1.6.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.1" rowspan="4"><span class="ltx_text" id="S5.T1.5.5.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="72" id="S5.T1.5.5.1.1.g1" src="extracted/5727909/room_w_double_door.png" width="90"/></span></td>
<td class="ltx_td ltx_border_t" id="S5.T1.6.6.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T1.6.6.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.6.6.2" rowspan="4"><span class="ltx_text" id="S5.T1.6.6.2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="72" id="S5.T1.6.6.2.1.g1" src="extracted/5727909/small_object_with_circle_v2.png" width="90"/></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.16">
<td class="ltx_td ltx_align_center" id="S5.T1.14.16.1">Take me to</td>
<td class="ltx_td ltx_align_center" id="S5.T1.14.16.2">Where can I</td>
<td class="ltx_td" id="S5.T1.14.16.3"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.17">
<td class="ltx_td ltx_align_center" id="S5.T1.14.17.1">a conference room</td>
<td class="ltx_td ltx_align_center" id="S5.T1.14.17.2">borrow a hand</td>
<td class="ltx_td" id="S5.T1.14.17.3"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.18">
<td class="ltx_td ltx_align_center" id="S5.T1.14.18.1">with a double door.</td>
<td class="ltx_td ltx_align_center" id="S5.T1.14.18.2">sanitizer?</td>
<td class="ltx_td" id="S5.T1.14.18.3"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.19">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.14.19.1"><span class="ltx_text ltx_font_bold" id="S5.T1.14.19.1.1">Reasoning-Required (RR)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S5.T1.14.19.2"><span class="ltx_text ltx_font_bold" id="S5.T1.14.19.2.1">Multimodal (MM)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.11.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.1">Text instr. (<math alttext="d" class="ltx_Math" display="inline" id="S5.T1.7.7.1.m1.1"><semantics id="S5.T1.7.7.1.m1.1a"><mi id="S5.T1.7.7.1.m1.1.1" xref="S5.T1.7.7.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.1b"><ci id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.1.m1.1d">italic_d</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.8.8.2">Goal frame (<math alttext="g" class="ltx_Math" display="inline" id="S5.T1.8.8.2.m1.1"><semantics id="S5.T1.8.8.2.m1.1a"><mi id="S5.T1.8.8.2.m1.1.1" xref="S5.T1.8.8.2.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.2.m1.1b"><ci id="S5.T1.8.8.2.m1.1.1.cmml" xref="S5.T1.8.8.2.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.2.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.2.m1.1d">italic_g</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.3">Text instr. (<math alttext="d" class="ltx_Math" display="inline" id="S5.T1.9.9.3.m1.1"><semantics id="S5.T1.9.9.3.m1.1a"><mi id="S5.T1.9.9.3.m1.1.1" xref="S5.T1.9.9.3.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.3.m1.1b"><ci id="S5.T1.9.9.3.m1.1.1.cmml" xref="S5.T1.9.9.3.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.3.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.9.3.m1.1d">italic_d</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.10.10.4">Image instr. (<math alttext="I" class="ltx_Math" display="inline" id="S5.T1.10.10.4.m1.1"><semantics id="S5.T1.10.10.4.m1.1a"><mi id="S5.T1.10.10.4.m1.1.1" xref="S5.T1.10.10.4.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.4.m1.1b"><ci id="S5.T1.10.10.4.m1.1.1.cmml" xref="S5.T1.10.10.4.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.4.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S5.T1.10.10.4.m1.1d">italic_I</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.11.11.5">Goal frame (<math alttext="g" class="ltx_Math" display="inline" id="S5.T1.11.11.5.m1.1"><semantics id="S5.T1.11.11.5.m1.1a"><mi id="S5.T1.11.11.5.m1.1.1" xref="S5.T1.11.11.5.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.5.m1.1b"><ci id="S5.T1.11.11.5.m1.1.1.cmml" xref="S5.T1.11.11.5.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.5.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S5.T1.11.11.5.m1.1d">italic_g</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.14">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.4">I want to store</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.12.12.1" rowspan="5"><span class="ltx_text" id="S5.T1.12.12.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="72" id="S5.T1.12.12.1.1.g1" src="extracted/5727909/cabinets.png" width="90"/></span></td>
<td class="ltx_td ltx_border_t" id="S5.T1.14.14.5"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.13.13.2" rowspan="5"><span class="ltx_text" id="S5.T1.13.13.2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="72" id="S5.T1.13.13.2.1.g1" src="extracted/5727909/return_box1_final.jpg" width="90"/></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.3" rowspan="4"><span class="ltx_text" id="S5.T1.14.14.3.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="72" id="S5.T1.14.14.3.1.g1" src="extracted/5727909/return_box_goal.png" width="90"/></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.20">
<td class="ltx_td ltx_align_center" id="S5.T1.14.20.1">something</td>
<td class="ltx_td ltx_align_center" id="S5.T1.14.20.2">Where should</td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.21">
<td class="ltx_td ltx_align_center" id="S5.T1.14.21.1">out of sight</td>
<td class="ltx_td ltx_align_center" id="S5.T1.14.21.2">I return this?</td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.22">
<td class="ltx_td ltx_align_center" id="S5.T1.14.22.1">from public eyes.</td>
<td class="ltx_td" id="S5.T1.14.22.2"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.23">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.14.23.1">Where should I go?</td>
<td class="ltx_td ltx_border_bb" id="S5.T1.14.23.2"></td>
<td class="ltx_td ltx_border_bb" id="S5.T1.14.23.3"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of user instructions in 4 categories (better in color).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">We highlight the key experimental setup below and leave details to Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7" title="7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Environments.</span>
We evaluate <span class="ltx_ERROR undefined" id="S5.p4.1.2">\method</span> for MINT in an real office environment occupied by humans (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.F2.sf1" title="In Figure 2 ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">2(a)</span></a>).
It is 836m<sup class="ltx_sup" id="S5.p4.1.3">2</sup> and cluttered with everyday items such as shelves, desks and chairs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.3"><span class="ltx_text ltx_font_bold" id="S5.p5.3.1">Robot.</span> We use a wheel-based mobile manipulator
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.F2.sf2" title="In Figure 2 ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">2(b)</span></a>) to evaluate <span class="ltx_ERROR undefined" id="S5.p5.3.2">\method</span>.
The robot uses a MPC-based algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib23" title="">23</a>]</cite> to execute the waypoint action (<math alttext="\Delta x" class="ltx_Math" display="inline" id="S5.p5.1.m1.1"><semantics id="S5.p5.1.m1.1a"><mrow id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mi id="S5.p5.1.m1.1.1.2" mathvariant="normal" xref="S5.p5.1.m1.1.1.2.cmml">Δ</mi><mo id="S5.p5.1.m1.1.1.1" xref="S5.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><times id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1"></times><ci id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2">Δ</ci><ci id="S5.p5.1.m1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">\Delta x</annotation><annotation encoding="application/x-llamapun" id="S5.p5.1.m1.1d">roman_Δ italic_x</annotation></semantics></math>, <math alttext="\Delta y" class="ltx_Math" display="inline" id="S5.p5.2.m2.1"><semantics id="S5.p5.2.m2.1a"><mrow id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml"><mi id="S5.p5.2.m2.1.1.2" mathvariant="normal" xref="S5.p5.2.m2.1.1.2.cmml">Δ</mi><mo id="S5.p5.2.m2.1.1.1" xref="S5.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.p5.2.m2.1.1.3" xref="S5.p5.2.m2.1.1.3.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><apply id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1"><times id="S5.p5.2.m2.1.1.1.cmml" xref="S5.p5.2.m2.1.1.1"></times><ci id="S5.p5.2.m2.1.1.2.cmml" xref="S5.p5.2.m2.1.1.2">Δ</ci><ci id="S5.p5.2.m2.1.1.3.cmml" xref="S5.p5.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">\Delta y</annotation><annotation encoding="application/x-llamapun" id="S5.p5.2.m2.1d">roman_Δ italic_y</annotation></semantics></math>, <math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S5.p5.3.m3.1"><semantics id="S5.p5.3.m3.1a"><mrow id="S5.p5.3.m3.1.1" xref="S5.p5.3.m3.1.1.cmml"><mi id="S5.p5.3.m3.1.1.2" mathvariant="normal" xref="S5.p5.3.m3.1.1.2.cmml">Δ</mi><mo id="S5.p5.3.m3.1.1.1" xref="S5.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.p5.3.m3.1.1.3" xref="S5.p5.3.m3.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.3.m3.1b"><apply id="S5.p5.3.m3.1.1.cmml" xref="S5.p5.3.m3.1.1"><times id="S5.p5.3.m3.1.1.1.cmml" xref="S5.p5.3.m3.1.1.1"></times><ci id="S5.p5.3.m3.1.1.2.cmml" xref="S5.p5.3.m3.1.1.2">Δ</ci><ci id="S5.p5.3.m3.1.1.3.cmml" xref="S5.p5.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.3.m3.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="S5.p5.3.m3.1d">roman_Δ italic_θ</annotation></semantics></math> in the robot-centric frame) while avoiding obstacles.</p>
</div>
<figure class="ltx_figure" id="S5.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="146" id="S5.F2.sf1.g1" src="extracted/5727909/top_down/longest_plus_all_routes.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="133" id="S5.F2.sf2.g1" src="extracted/5727909/edr_robot.jpg" width="75"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Experiment setup.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1"><span class="ltx_text ltx_font_bold" id="S5.p6.1.1">Demonstration Tour.</span> We collect the demonstration tour by teleoperating the robot with a gamepad.
All corridors are traversed twice from opposite directions.
The resulting tour is roughly 16 minutes long (948 frames @ 1Hz) and we add narratives during the tour “Temp desk for everyone” and “Lewis’ desk” to frame 5:28 and 7:14 respectively to enable personalized navigation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p7">
<p class="ltx_p" id="S5.p7.1"><span class="ltx_text ltx_font_bold" id="S5.p7.1.1">Multimodal User Instructions.</span>
We crowd-sourced 57 user instructions in 4 categories.
This includes:
20 Reasoning-Free (RF), 15 Reasoning-Required (RR), 12 Small Objects (SO), and 10 Multimodal (MM) instructions (Examples are in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T1" title="Table 1 ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a>, full list in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS7" title="7.7 User Instructions tested in the Office environment ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.7</span></a>).
Importantly, “Reasoning Required” instructions do not mention the specific object or location the robot needs to navigate to, and the destination of Multimodal instructions are nearly impossible to infer without the image modality in the instruction.
As far as we know, prior works were not designed for or evaluated against these two categories of tasks, and they are the key differentiator between MINT and ObjNav and VLN.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span class="ltx_text ltx_font_bold" id="S5.SS1.1.1">RQ1:</span> <span class="ltx_ERROR undefined" id="S5.SS1.2.2">\method</span>’s robust high end-to-end performance in the wild</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To evaluate <span class="ltx_ERROR undefined" id="S5.SS1.p1.1.1">\method</span> in MINT in the real world, we randomly select 5 user instructions per category and evaluate <span class="ltx_ERROR undefined" id="S5.SS1.p1.1.2">\method</span>’s performance from 4 random starting poses (location and yaw) that are at least 20 m away.
We use Gemini 1.5 Pro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib17" title="">17</a>]</cite> as our long-context multimodal VLM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">High end-to-end success rate.</span> Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T2" title="Table 2 ‣ 5.1 RQ1: \method’s robust high end-to-end performance in the wild ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">2</span></a> shows that <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.2">\method</span> has a high end-to-end navigation success rate in most user instructions categories, including previously infeasible Reasoning-Required and Multimodal instructions.
However, the success rate is significantly lower in the Small Object category.
This is not unexpected given the limited tour video resolution.
<span class="ltx_ERROR undefined" id="S5.SS1.p2.1.3">\method</span> also has a reasonable SPL (Success Rate weighted Path Length), indicating that the topological graph does not incur a high path length penalty.
Lastly, <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.4">\method</span> successfully incorporated the personalization narratives in the demonstration tour.
It correctly navigated to different locations when responding to essentially the same instructions, but from different users (moved to frame 7:14 when asked “I’m Lewis, take me to a temp desk please.” and moved to frame 5:28 when asked “Hi robot, I’m visiting, can you take me to a temp desk?”).
See the supplementary video for examples.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.1">
<tr class="ltx_tr" id="S5.T2.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T2.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.2">Reasoning-Free</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.3">Reasoning-Required</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.4">Small Objects</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.5">Multimodal</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1">Goal Finding SR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2">80%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.3">80%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.4">40%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.5">85%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3">
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.1">Goal Reaching SR</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2">100%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3">100%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.4">100%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.5">100%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4">
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.1">End-to-end SR</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.2">80%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3">80%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4">40%</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.5">85%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.1">SPL</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.2">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.3">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4">0.38</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.5">0.64</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_ERROR undefined" id="S5.T2.4.1">\method</span> end-to-end navigation Success Rate (SR) and SPL of various user instruction types in the <span class="ltx_text ltx_font_italic" id="S5.T2.5.2">real</span> Office environment.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Robust low-level goal reaching.</span> Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T2" title="Table 2 ‣ 5.1 RQ1: \method’s robust high end-to-end performance in the wild ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">2</span></a> also shows the robustness of <span class="ltx_ERROR undefined" id="S5.SS1.p3.1.2">\method</span>’s low-level goal reaching policy (100% success rate) in the real world, with the demonstration tour recorded months prior to experiments when many objects, furniture, and lighting conditions had been different.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Large-scale sim confirms high end-to-end success rate.</span>
To further investigate the end-to-end performance, we leverage simulations to scale evaluation numbers. Concretely, we created a high fidelity simulation reconstruction of the office environment using NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib56" title="">56</a>]</cite> (see Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS7" title="7.7 User Instructions tested in the Office environment ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.7</span></a> for details and example images), and evaluate <span class="ltx_ERROR undefined" id="S5.SS1.p4.1.2">\method</span> against 20 language instructed tasks with 50 random starting poses per task.
Our experiment resulted in 90% high level goal finding and 100% low level goal reaching success rates, with a total of 900 successful end-to-end execution (full results in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.T8" title="Table 8 ‣ 7.4 Large-Scale end-to-end sim details ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">8</span></a> in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS3" title="7.3 Additional Experiments ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.3</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">Generalization to a home-like environment using a smartphone tour.</span>
To show <span class="ltx_ERROR undefined" id="S5.SS1.p5.1.2">\method</span>’s generality and ease of use, we conducted a proof-of-concept experiment in a real home-like environment (details in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS2" title="7.2 Home-Like Environment Experiment Setup ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.2</span></a>).
Rather than giving the robot a teleoperated tour, we use a <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.3">Google Pixel 6 smartphone to record the demonstration tour</span>, and then evaluated <span class="ltx_ERROR undefined" id="S5.SS1.p5.1.4">\method</span> end-to-end with 4 Reasoning-Required and 1 Small Object user instructions with 4 random starts each.
The success rate is 100% with a SPL of 0.87.
This shows that 1) <span class="ltx_ERROR undefined" id="S5.SS1.p5.1.5">\method</span> performs well regardless of environments, and 2) it is extremely easy to deploy, as the user can simply use their smartphone to record a tour of their environment, upload to the robot and then immediately start giving instructions.
To our knowledge, this level of generality and ease of use had never been shown in robot navigation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>RQ2: Long-context VLM outperforms alternatives on high level goal finding</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We investigate how well alternative methods perform compared to <span class="ltx_ERROR undefined" id="S5.SS2.p1.1.1">\method</span> to answer whether using long-context multimodal VLMs is the key to solve MINT. Concretely, we compare the following baselines:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<ol class="ltx_enumerate" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<div class="ltx_para ltx_noindent" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">CLIP-based retrieval</span>: We reproduce the high-level goal finding module of NLMap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib14" title="">14</a>]</cite> by adopting OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib57" title="">57</a>]</cite> for region proposal and CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib58" title="">58</a>]</cite> for sub-regions and full-images embeddings extraction for tour frames. We then perform goal frame retrieval using CLIP embeddings of the instruction language and image. State-of-the-art work like ESC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib26" title="">26</a>]</cite>, ZSON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib35" title="">35</a>]</cite>, and CLIP-on-Wheels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib59" title="">59</a>]</cite> also fall into the category of this baseline.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<div class="ltx_para ltx_noindent" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">Text-Only <span class="ltx_ERROR undefined" id="S5.I2.i2.p1.1.1.1">\method</span></span>: Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib60" title="">60</a>]</cite>, where the multimodal demonstration tour is captioned by a VLM frame-by-frame to form a “text tour”. An LLM (Gemini 1.5 Pro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib16" title="">16</a>]</cite>) then uses the text tour to produce the goal frame index.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<span class="ltx_ERROR undefined" id="S5.SS2.p3.1">\method</span>
<p class="ltx_p" id="S5.SS2.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.2.1">outperforms comparisons.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T3" title="Table 3 ‣ 5.2 RQ2: Long-context VLM outperforms alternatives on high level goal finding ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">3</span></a> shows that high-level goal finding success rates of <span class="ltx_ERROR undefined" id="S5.SS2.p3.2.2">\method</span> are significantly higher than comparison methods.
Given the 100% low-level success rate, this high-level goal finding success rates are representative of end-to-end success rates.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.1">
<tr class="ltx_tr" id="S5.T3.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1">Success Rates</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.2">Reasoning-Free</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.3">Reasoning Required</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.4">Small Objects</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.5">Multimodal</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1">CLIP-based retrieval</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.2">35%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.3">33%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.4">25%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.5">20%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3">
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.1">Text Only <span class="ltx_ERROR undefined" id="S5.T3.1.3.1.1">\method</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2">70%</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3">60%</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.4">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.3.4.1">50</span>%</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.5">30%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.1">
<span class="ltx_ERROR undefined" id="S5.T3.1.4.1.1">\method</span> (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.2.1">95%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.3.1">86%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.4">42%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.5.1">90%</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>High-level goal finding Success Rates of <span class="ltx_ERROR undefined" id="S5.T3.3.1">\method</span> compared to baselines</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Processing high frame rate tour videos with long-context VLMs is critical for success.</span>
Feeding a full demonstration tour of a large environment into non-long-context VLMs is challenging since each image requires hundreds-of-token budgets.
One solution for reducing input tokens number is <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.2">reducing</span> the tour video frame rate, at the cost of intermediate frames loss.
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T4" title="Table 4 ‣ 5.2 RQ2: Long-context VLM outperforms alternatives on high level goal finding ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the high-level goal finding success rate decreases as the tour frame rate decreases.
This is unsurprising since a lower frame rate tour can sometimes miss the navigation target frame.
In addition, comparing state of the art VLMs, only Gemini 1.5 Pro yields satisfactory success rate thanks to its long 1M token context-length.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T4.1">
<tr class="ltx_tr" id="S5.T4.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1">Frame</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T4.1.1.2">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib18" title="">18</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T4.1.1.3">GPT-4o <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib61" title="">61</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T4.1.1.4">Gemini 1.5 Pro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib17" title="">17</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2">
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1">Rate</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.2">RF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.3">RR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.4">SO</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.5">MM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.6">RF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.7">RR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.8">SO</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.9">MM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.10">RF</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.11">RR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.12">SO</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.13">MM</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.1">0.2 FPS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2">60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.3">53%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.4">17%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.5">30%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.6">75%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.7">40%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.8">25%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.9">50%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.10">95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.11">67%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.12">36%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.13">60%</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.4.1">1 FPS</td>
<td class="ltx_td ltx_align_center ltx_border_bb" colspan="4" id="S5.T4.1.4.2">Exceeds token limit</td>
<td class="ltx_td ltx_align_center ltx_border_bb" colspan="4" id="S5.T4.1.4.3">Exceeds token limit</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.4.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.4.1">95%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.4.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.5.1">86%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.4.6"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.6.1">42%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.4.7"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.7.1">90%</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>High-level goal finding Success Rates with regards to various user instruction types (presented in the order of Reasoning Free (RF), Reasoning Required (RR), Small Objects (SO), MultiModal (MM)) as a function of VLM models (column) and multimodal demonstration tour Frames Per Second (FPS) (row). All VLMs were queried in June 2024.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">We also showcase one selected qualitative comparison example for high-level goal finding of all candidates approaches in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.F3" title="Figure 3 ‣ 5.2 RQ2: Long-context VLM outperforms alternatives on high level goal finding ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">3</span></a>.
When given the multimodal instruction of “I want more of this.” and a picture of several Coke cans on a desk, Mobility VLA correctly identified the frame containing the refrigerator which it should lead the user to. On the other hand, CLIP-based retrieval finds a region in which a water bottle and some stuff are on a desk to be most similar to the full instruction image, given it is hard to extract “what the user want” from the instruction image using Owl-ViT. GPT-4o incorrectly attempts to find the frame closest to the instruction image, while GPT-4V refuses to give a frame number since it was unable to find a frame where beverages are. Lastly, the Text only approach cannot understand whether “this” refers to the Coke cans or the office setting, since it relies only on caption of the instruction image.
The full LLM responses can be found in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS5" title="7.5 Full VLM intermediate output in Figure 3 ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.5</span></a></p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="120" id="S5.F3.g1" src="extracted/5727909/qualitative_comparison_v3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualitative comparison of <span class="ltx_ERROR undefined" id="S5.F3.2.1">\method</span> and other approaches on a multimodal instruction. The bottom row shows the intermediate output of each approach.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1">Altogether, experiments in this section show that the <span class="ltx_text ltx_font_italic" id="S5.SS2.p6.1.1">long-context multimodal VLM capability is critical in solving MINT.</span>
In addition, at present, only the Gemini 1.5 Pro VLM provides satisfactory success rate due to its long 1M token context-length.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>RQ3: Topological graphs are critical for success</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<span class="ltx_ERROR undefined" id="S5.SS3.p1.1">\method</span>
<p class="ltx_p" id="S5.SS3.p1.2">uses a hierarchical architecture to harness long-context VLM’s reasoning capability and uses a topological graph to produce waypoint actions.
Is this necessary? Can we prompt the VLM to output waypoint actions directly?</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Topological graphs are critical for navigation success.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.T5" title="Table 5 ‣ 5.3 RQ3: Topological graphs are critical for success ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">5</span></a> shows the end-to-end performance of <span class="ltx_ERROR undefined" id="S5.SS3.p2.1.2">\method</span> in simulation compared to prompting the VLM to output waypoint actions directly (prompt and details in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.SS6" title="7.6 Prompt for direct VLM waypoint actions output ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7.6</span></a>).
The 0% end-to-end success rate shows that Gemini 1.5 Pro is incapable of navigating the robot zero-shot w/o the topological graph.
Empirically, we found that Gemini almost always outputs the “move forward” waypoint action regardless of the current camera observation.
In addition, the current Gemini 1.5 API requires the upload of all 948 tour images at every inference call, resulting in a prohibitively expensive 26s per-step running time for the robot to move just 1m.
On the other hand, <span class="ltx_ERROR undefined" id="S5.SS3.p2.1.3">\method</span>’s high-level VLM spends 10-30s to find a goal index and then the robot navigates to the goal using the low-level topological graph results in a highly robust and efficient (0.19s per step) system for solving MINT.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.2">
<tr class="ltx_tr" id="S5.T5.2.3">
<td class="ltx_td ltx_border_tt" id="S5.T5.2.3.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.2.3.2">Direct Waypoint Action Output</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.2.3.3">Goal Index Output + Topological Graph</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.4.1">Success Rate</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.4.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.4.3"><span class="ltx_text ltx_font_bold" id="S5.T5.2.4.3.1">90%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.5">
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.1">SPL</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.2">-</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.5.3">0.84</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.2.2.3">Per-step inference Time</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.1">25.90<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T5.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.m1.1d">±</annotation></semantics></math>8.36s</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T5.2.2.2.1">0.19<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T5.2.2.2.1.m1.1"><semantics id="S5.T5.2.2.2.1.m1.1a"><mo id="S5.T5.2.2.2.1.m1.1.1" xref="S5.T5.2.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S5.T5.2.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.1.m1.1d">±</annotation></semantics></math>0.047s</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>End-to-end navigation Success Rate and SPL as a function of VLM (Gemini 1.5 Pro) output format in the <span class="ltx_text ltx_font_italic" id="S5.T5.4.1">simulated</span> Office environment.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we present <span class="ltx_ERROR undefined" id="S6.p1.1.1">\method</span>, a new paradigm of navigation policy for solving MINT.
<span class="ltx_ERROR undefined" id="S6.p1.1.2">\method</span> achieved 86% and 90% end-to-end success rates on previously infeasible navigation tasks involving complex reasoning and multimodal user instructions in a large real world environment.
We also demonstrated a leap forward in how easily users can interact with the robot,
where a user records a video walkthrough in a home environment with a smartphone and then asks “Where did I leave my coaster?”</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Limitation: Lack of exploration.</span>
The current version of <span class="ltx_ERROR undefined" id="S6.p2.1.2">\method</span> relies on a demonstration tour, and does not explore the environment automatically.
However, existing exploration mechanisms such as frontier exploration or diffusion-based exploration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib43" title="">43</a>]</cite> can be easily integrated during the demonstration tour.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Limitation: Long VLM inference time impedes natural user interactions.</span>
The inference time of high-level VLMs is round 10-30 seconds, resulting in users awkwardly waiting for the robot to respond.
However, it is possible to cache the demonstration tour, which takes up roughly 99.9% of the input tokens, in order to significantly improve inference speed.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Future Work</span>.
<span class="ltx_ERROR undefined" id="S6.p4.1.2">\method</span> can be easily deployed on different robot embodiments as the onboard compute requirement is light (VLMs run on clouds) and only requires RGB camera observations.
We plan to deploy <span class="ltx_ERROR undefined" id="S6.p4.1.3">\method</span> on more embodiments.
In addition, we found preliminary evidence that Gemini 1.5 Pro is capable of generating high-level action plans to fulfill compound user multimodal instructions beyond navigation such as “Do they have my favorite drink today?” from a user with lots of Coke cans on their desk asks.
It knows that the robot should navigate to the fridge, inspect if there are Cokes, and then return to the user to report the result.
We included this result in the supplementary video and plan to investigate further.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Anthony Brohan, Bauyrjan Jyenis, Coline Devin, Dmitry Kalashnikov, Dorsa Sadigh, Joy Ortiz, Kuang-Huei Lee, Matthew Mounteer, Nikhil J Joshi, Peter Anderson, Quan Vuong, Robert Baruch, Sergey Yaroshenko, Sherry Moore, Tianli Ding, Ying Xu, Yixin Lin and Yuheng Kuang for their support, advice, comments and discussions during the project.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2018]</span>
<span class="ltx_bibblock">
Z. Xu, C. Tang, and M. Tomizuka.

</span>
<span class="ltx_bibblock">Zero-shot deep reinforcement learning driving policy transfer for
autonomous vehicles based on robust control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2018 21st International Conference on Intelligent
Transportation Systems (ITSC)</em>, pages 2865–2871. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2019]</span>
<span class="ltx_bibblock">
C. Tang, Z. Xu, and M. Tomizuka.

</span>
<span class="ltx_bibblock">Disturbance-observer-based tracking controller for neural network
driving policy transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Transactions on Intelligent Transportation Systems</em>,
21(9):3961–3972, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2019]</span>
<span class="ltx_bibblock">
Z. Xu, H. Chang, C. Tang, C. Liu, and M. Tomizuka.

</span>
<span class="ltx_bibblock">Toward modularization of neural network autonomous driving policy
using parallel attribute networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2019 IEEE Intelligent Vehicles Symposium (IV)</em>, pages
1400–1407. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. [2020]</span>
<span class="ltx_bibblock">
H. Chang, Z. Xu, and M. Tomizuka.

</span>
<span class="ltx_bibblock">Cascade attribute network: Decomposing reinforcement learning control
policies using hierarchical neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IFAC-PapersOnLine</em>, 53(2):8181–8186, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2022]</span>
<span class="ltx_bibblock">
R. Zhou, H. Zhou, H. Gao, M. Tomizuka, J. Li, and Z. Xu.

</span>
<span class="ltx_bibblock">Grouptron: Dynamic multi-scale graph convolutional networks for
group-aware dense crowd trajectory forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2022 International Conference on Robotics and Automation
(ICRA)</em>, pages 805–811. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023]</span>
<span class="ltx_bibblock">
L. Sun, P.-Y. Hung, C. Wang, M. Tomizuka, and Z. Xu.

</span>
<span class="ltx_bibblock">Distributed multi-agent interaction generation with imagined
potential games.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2310.01614</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2024]</span>
<span class="ltx_bibblock">
Z. Xu, R. Zhou, Y. Yin, H. Gao, M. Tomizuka, and J. Li.

</span>
<span class="ltx_bibblock">Matrix: Multi-agent trajectory generation with diverse contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2403.06041</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. [2018]</span>
<span class="ltx_bibblock">
P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun,
J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al.

</span>
<span class="ltx_bibblock">On evaluation of embodied navigation agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1807.06757</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Batra et al. [2020]</span>
<span class="ltx_bibblock">
D. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva,
A. Toshev, and E. Wijmans.

</span>
<span class="ltx_bibblock">Objectnav revisited: On evaluation of embodied agents navigating to
objects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2006.13171</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2022]</span>
<span class="ltx_bibblock">
J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. E. Wang.

</span>
<span class="ltx_bibblock">Vision-and-language navigation: A survey of tasks, methods, and
future directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2203.12667</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. [2018]</span>
<span class="ltx_bibblock">
P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid,
S. Gould, and A. Van Den Hengel.

</span>
<span class="ltx_bibblock">Vision-and-language navigation: Interpreting visually-grounded
navigation instructions in real environments.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 3674–3683, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2019]</span>
<span class="ltx_bibblock">
H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi.

</span>
<span class="ltx_bibblock">Touchdown: Natural language navigation and spatial reasoning in
visual street environments.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 12538–12547, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2017]</span>
<span class="ltx_bibblock">
Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi.

</span>
<span class="ltx_bibblock">Target-driven visual navigation in indoor scenes using deep
reinforcement learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2017 IEEE international conference on robotics and
automation (ICRA)</em>, pages 3357–3364. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone,
and D. Kappler.

</span>
<span class="ltx_bibblock">Open-vocabulary queryable scene representations for real world
planning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2023 IEEE International Conference on Robotics and
Automation (ICRA)</em>, pages 11509–11522. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ANYbotics [2024]</span>
<span class="ltx_bibblock">
ANYbotics.

</span>
<span class="ltx_bibblock">Automate industrial inspection with anymal.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anybotics.com/robotics/automate-inspection/" title="">https://www.anybotics.com/robotics/automate-inspection/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team Google [2023]</span>
<span class="ltx_bibblock">
Gemini Team Google.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. [2024]</span>
<span class="ltx_bibblock">
M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac,
R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. [2023]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024]</span>
<span class="ltx_bibblock">
B. Chen, Z. Xu, S. Kirmani, B. Ichter, D. Driess, P. Florence, D. Sadigh,
L. Guibas, and F. Xia.

</span>
<span class="ltx_bibblock">Spatialvlm: Endowing vision-language models with spatial reasoning
capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2401.12168</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marder-Eppstein et al. [2010]</span>
<span class="ltx_bibblock">
E. Marder-Eppstein, E. Berger, T. Foote, B. Gerkey, and K. Konolige.

</span>
<span class="ltx_bibblock">The office marathon: Robust navigation in an indoor office
environment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2010 IEEE international conference on robotics and
automation</em>, pages 300–307. IEEE, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferguson and Stentz [2006]</span>
<span class="ltx_bibblock">
D. Ferguson and A. Stentz.

</span>
<span class="ltx_bibblock">Using interpolation to improve path planning: The field d* algorithm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Journal of Field Robotics</em>, 23(2):79–101,
2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schönberger and Frahm [2016]</span>
<span class="ltx_bibblock">
J. L. Schönberger and J.-M. Frahm.

</span>
<span class="ltx_bibblock">Structure-from-motion revisited.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Frostig, V. Sindhwani, S. Singh, and S. Tu.

</span>
<span class="ltx_bibblock">trajax: differentiable optimal control on accelerators, 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">URL http://github. com/google/trajax</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaplot et al. [2020]</span>
<span class="ltx_bibblock">
D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov.

</span>
<span class="ltx_bibblock">Object goal navigation using goal-oriented semantic exploration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>,
33:4247–4258, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre et al. [2022]</span>
<span class="ltx_bibblock">
S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song.

</span>
<span class="ltx_bibblock">Clip on wheels: Zero-shot object navigation as object localization
and exploration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2203.10421</em>, 3(4):7,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2023]</span>
<span class="ltx_bibblock">
K. Zhou, K. Zheng, C. Pryor, Y. Shen, H. Jin, L. Getoor, and X. E. Wang.

</span>
<span class="ltx_bibblock">Esc: Exploration with soft commonsense constraints for zero-shot
object navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International Conference on Machine Learning</em>, pages
42829–42842. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maksymets et al. [2021]</span>
<span class="ltx_bibblock">
O. Maksymets, V. Cartillier, A. Gokaslan, E. Wijmans, W. Galuba, S. Lee, and
D. Batra.

</span>
<span class="ltx_bibblock">Thda: Treasure hunt data augmentation for semantic navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 15374–15383, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramrakhya et al. [2022]</span>
<span class="ltx_bibblock">
R. Ramrakhya, E. Undersander, D. Batra, and A. Das.

</span>
<span class="ltx_bibblock">Habitat-web: Learning embodied object-search strategies from human
demonstrations at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 5173–5183, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavian et al. [2019]</span>
<span class="ltx_bibblock">
A. Mousavian, A. Toshev, M. Fišer, J. Košecká, A. Wahid, and
J. Davidson.

</span>
<span class="ltx_bibblock">Visual representations for semantic target driven navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">2019 International Conference on Robotics and Automation
(ICRA)</em>, pages 8846–8852. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ku et al. [2020]</span>
<span class="ltx_bibblock">
A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge.

</span>
<span class="ltx_bibblock">Room-across-room: Multilingual vision-and-language navigation with
dense spatiotemporal grounding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2010.07954</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moudgil et al. [2021]</span>
<span class="ltx_bibblock">
A. Moudgil, A. Majumdar, H. Agrawal, S. Lee, and D. Batra.

</span>
<span class="ltx_bibblock">Soat: A scene-and object-aware transformer for vision-and-language
navigation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>,
34:7357–7367, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomason et al. [2020]</span>
<span class="ltx_bibblock">
J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer.

</span>
<span class="ltx_bibblock">Vision-and-dialog navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Conference on Robot Learning</em>, pages 394–406. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mezghan et al. [2022]</span>
<span class="ltx_bibblock">
L. Mezghan, S. Sukhbaatar, T. Lavril, O. Maksymets, D. Batra, P. Bojanowski,
and K. Alahari.

</span>
<span class="ltx_bibblock">Memory-augmented reinforcement learning for image-goal navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">2022 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</em>, pages 3316–3323. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sridhar et al. [2023]</span>
<span class="ltx_bibblock">
A. Sridhar, D. Shah, C. Glossop, and S. Levine.

</span>
<span class="ltx_bibblock">Nomad: Goal masked diffusion policies for navigation and exploration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2310.07896</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumdar et al. [2022]</span>
<span class="ltx_bibblock">
A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra.

</span>
<span class="ltx_bibblock">Zson: Zero-shot object-goal navigation using multimodal goal
embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems</em>,
35:32340–32352, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savinov et al. [2018]</span>
<span class="ltx_bibblock">
N. Savinov, A. Dosovitskiy, and V. Koltun.

</span>
<span class="ltx_bibblock">Semi-parametric topological memory for navigation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. [2019]</span>
<span class="ltx_bibblock">
K. Fang, A. Toshev, L. Fei-Fei, and S. Savarese.

</span>
<span class="ltx_bibblock">Scene memory transformer for embodied agents in long-horizon tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 538–547, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eysenbach et al. [2019]</span>
<span class="ltx_bibblock">
B. Eysenbach, R. R. Salakhutdinov, and S. Levine.

</span>
<span class="ltx_bibblock">Search on the replay buffer: Bridging planning and reinforcement
learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
X. Li, D. Guo, H. Liu, and F. Sun.

</span>
<span class="ltx_bibblock">Embodied semantic scene graph generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Conference on robot learning</em>, pages 1585–1594. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wald et al. [2020]</span>
<span class="ltx_bibblock">
J. Wald, H. Dhamo, N. Navab, and F. Tombari.

</span>
<span class="ltx_bibblock">Learning 3d semantic scene graphs from 3d indoor reconstructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 3961–3970, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yadav et al. [2023]</span>
<span class="ltx_bibblock">
K. Yadav, R. Ramrakhya, A. Majumdar, V.-P. Berges, S. Kuhar, D. Batra,
A. Baevski, and O. Maksymets.

</span>
<span class="ltx_bibblock">Offline visual representation learning for embodied navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Workshop on Reincarnating Reinforcement Learning at ICLR
2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. [2020]</span>
<span class="ltx_bibblock">
M. Chang, A. Gupta, and S. Gupta.

</span>
<span class="ltx_bibblock">Semantic visual navigation by watching youtube videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Advances in Neural Information Processing Systems</em>,
33:4283–4294, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. [2023a]</span>
<span class="ltx_bibblock">
D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and
S. Levine.

</span>
<span class="ltx_bibblock">Vint: A foundation model for visual navigation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2306.14846</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. [2023b]</span>
<span class="ltx_bibblock">
D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine.

</span>
<span class="ltx_bibblock">Gnm: A general navigation model to drive any robot.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">2023 IEEE International Conference on Robotics and
Automation (ICRA)</em>, pages 7226–7233. IEEE, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al. [2023]</span>
<span class="ltx_bibblock">
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,
E. Taropa, P. Bailey, Z. Chen, et al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2305.10403</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dorbala et al. [2022]</span>
<span class="ltx_bibblock">
V. S. Dorbala, G. Sigurdsson, R. Piramuthu, J. Thomason, and G. S. Sukhatme.

</span>
<span class="ltx_bibblock">Clip-nav: Using clip for zero-shot vision-and-language navigation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2211.16649</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. [2023]</span>
<span class="ltx_bibblock">
D. Shah, M. R. Equi, B. Osiński, F. Xia, B. Ichter, and S. Levine.

</span>
<span class="ltx_bibblock">Navigation with large language models: Semantic guesswork as a
heuristic for planning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Conference on Robot Learning</em>, pages 2683–2699. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2024]</span>
<span class="ltx_bibblock">
G. Zhou, Y. Hong, and Q. Wu.

</span>
<span class="ltx_bibblock">Navgpt: Explicit reasoning in vision-and-language navigation with
large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 38, pages 7641–7649, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. [2023]</span>
<span class="ltx_bibblock">
D. Shah, B. Osiński, S. Levine, et al.

</span>
<span class="ltx_bibblock">Lm-nav: Robotic navigation with large pre-trained models of language,
vision, and action.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Conference on robot learning</em>, pages 492–504. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. [2023]</span>
<span class="ltx_bibblock">
J. Mao, Y. Qian, H. Zhao, and Y. Wang.

</span>
<span class="ltx_bibblock">Gpt-driver: Learning to drive with gpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2310.01415</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schönberger et al. [2016]</span>
<span class="ltx_bibblock">
J. L. Schönberger, E. Zheng, M. Pollefeys, and J.-M. Frahm.

</span>
<span class="ltx_bibblock">Pixelwise view selection for unstructured multi-view stereo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">European Conference on Computer Vision (ECCV)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavraki et al. [1998]</span>
<span class="ltx_bibblock">
L. E. Kavraki, M. N. Kolountzakis, and J.-C. Latombe.

</span>
<span class="ltx_bibblock">Analysis of probabilistic roadmaps for path planning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">IEEE Transactions on Robotics and automation</em>, 14(1):166–171, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2020]</span>
<span class="ltx_bibblock">
B. Cao, A. Araujo, and J. Sim.

</span>
<span class="ltx_bibblock">Unifying deep local and global features for image search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16</em>, pages 726–743.
Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartley and Zisserman [2003]</span>
<span class="ltx_bibblock">
R. Hartley and A. Zisserman.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Multiple view geometry in computer vision</em>.

</span>
<span class="ltx_bibblock">Cambridge university press, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
B. Mildenhall, P. Srinivasan, M. Tancik, J. Barron, R. Ramamoorthi, and R. N.
Nerf.

</span>
<span class="ltx_bibblock">Representing scenes as neural radiance fields for view synthesis.,
2021, 65.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">DOI: https://doi. org/10.1145/3503250</em>, pages 99–106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minderer et al. [2022]</span>
<span class="ltx_bibblock">
M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn,
A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al.

</span>
<span class="ltx_bibblock">Simple open-vocabulary object detection with vision transformers.
arxiv 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2205.06230</em>, 2, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">International conference on machine learning</em>, pages
8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre et al. [2023]</span>
<span class="ltx_bibblock">
S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song.

</span>
<span class="ltx_bibblock">Cows on pasture: Baselines and benchmarks for language-driven
zero-shot object navigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 23171–23181, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. [2023]</span>
<span class="ltx_bibblock">
W. Cai, S. Huang, G. Cheng, Y. Long, P. Gao, C. Sun, and H. Dong.

</span>
<span class="ltx_bibblock">Bridging zero-shot object navigation and foundation models through
pixel-guided navigation skill.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2309.10309</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2024]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Hello gpt-4o — openai, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/gpt-4o/" title="">https://openai.com/blog/gpt-4o/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neven et al. [2008]</span>
<span class="ltx_bibblock">
H. Neven, G. Rose, and W. G. Macready.

</span>
<span class="ltx_bibblock">Image recognition with an adiabatic quantum computer i. mapping to
quadratic unconstrained binary optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:0804.4457</em>, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barron et al. [2023]</span>
<span class="ltx_bibblock">
J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman.

</span>
<span class="ltx_bibblock">Zip-nerf: Anti-aliased grid-based neural radiance fields.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 19697–19705, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rakotosaona et al. [2023]</span>
<span class="ltx_bibblock">
M.-J. Rakotosaona, F. Manhardt, D. M. Arroyo, M. Niemeyer, A. Kundu, and
F. Tombari.

</span>
<span class="ltx_bibblock">Nerfmeshing: Distilling neural radiance fields into
geometrically-accurate 3d meshes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">International Conference on 3D Vision (3DV)</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Appendix</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Structure-from-Motion and Hierarchical Localization</h3>
<div class="ltx_para ltx_noindent" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">We use COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib22" title="">22</a>]</cite>, an off-the-shelf structure-from-motion pipeline to estimate the pose of the robot for each frame in the tour (i.e. reference images), 3D point landmarks in the environment (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.F4" title="Figure 4 ‣ 7.1 Structure-from-Motion and Hierarchical Localization ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">4</span></a>) and their corresponding 2D projections across all reference images (i.e. 2D-3D correspondences).</p>
</div>
<figure class="ltx_figure" id="S7.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="87" id="S7.F4.g1" src="extracted/5727909/sfm/colmap_view.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top-down view of the COLMAP result of the office environment: 3D point landmarks and reference image poses (blue).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.3">The poses are used to build a fully connected topological graph. The tour frames <math alttext="F" class="ltx_Math" display="inline" id="S7.SS1.p2.1.m1.1"><semantics id="S7.SS1.p2.1.m1.1a"><mi id="S7.SS1.p2.1.m1.1.1" xref="S7.SS1.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.1.m1.1b"><ci id="S7.SS1.p2.1.m1.1.1.cmml" xref="S7.SS1.p2.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p2.1.m1.1d">italic_F</annotation></semantics></math>, 3D landmarks and 2D features are used in our own implementation of a real-time hierarchical localizer. The method is hierarchical since it divides localization of the observed image <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p2.2.m2.1"><semantics id="S7.SS1.p2.2.m2.1a"><mi id="S7.SS1.p2.2.m2.1.1" xref="S7.SS1.p2.2.m2.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.2.m2.1b"><ci id="S7.SS1.p2.2.m2.1.1.cmml" xref="S7.SS1.p2.2.m2.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.2.m2.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p2.2.m2.1d">italic_O</annotation></semantics></math> into two steps: a global search to determine a set of candidate reference images close to <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p2.3.m3.1"><semantics id="S7.SS1.p2.3.m3.1a"><mi id="S7.SS1.p2.3.m3.1.1" xref="S7.SS1.p2.3.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.3.m3.1b"><ci id="S7.SS1.p2.3.m3.1.1.cmml" xref="S7.SS1.p2.3.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.3.m3.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p2.3.m3.1d">italic_O</annotation></semantics></math> followed by local feature matching and pose estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.6">In the global search, the candidate set <math alttext="C\subseteq F" class="ltx_Math" display="inline" id="S7.SS1.p3.1.m1.1"><semantics id="S7.SS1.p3.1.m1.1a"><mrow id="S7.SS1.p3.1.m1.1.1" xref="S7.SS1.p3.1.m1.1.1.cmml"><mi id="S7.SS1.p3.1.m1.1.1.2" xref="S7.SS1.p3.1.m1.1.1.2.cmml">C</mi><mo id="S7.SS1.p3.1.m1.1.1.1" xref="S7.SS1.p3.1.m1.1.1.1.cmml">⊆</mo><mi id="S7.SS1.p3.1.m1.1.1.3" xref="S7.SS1.p3.1.m1.1.1.3.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.1.m1.1b"><apply id="S7.SS1.p3.1.m1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1"><subset id="S7.SS1.p3.1.m1.1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1.1"></subset><ci id="S7.SS1.p3.1.m1.1.1.2.cmml" xref="S7.SS1.p3.1.m1.1.1.2">𝐶</ci><ci id="S7.SS1.p3.1.m1.1.1.3.cmml" xref="S7.SS1.p3.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.1.m1.1c">C\subseteq F</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.1.m1.1d">italic_C ⊆ italic_F</annotation></semantics></math> of k-nearest (w.r.t. the <math alttext="l^{2}" class="ltx_Math" display="inline" id="S7.SS1.p3.2.m2.1"><semantics id="S7.SS1.p3.2.m2.1a"><msup id="S7.SS1.p3.2.m2.1.1" xref="S7.SS1.p3.2.m2.1.1.cmml"><mi id="S7.SS1.p3.2.m2.1.1.2" xref="S7.SS1.p3.2.m2.1.1.2.cmml">l</mi><mn id="S7.SS1.p3.2.m2.1.1.3" xref="S7.SS1.p3.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.2.m2.1b"><apply id="S7.SS1.p3.2.m2.1.1.cmml" xref="S7.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S7.SS1.p3.2.m2.1.1.1.cmml" xref="S7.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S7.SS1.p3.2.m2.1.1.2.cmml" xref="S7.SS1.p3.2.m2.1.1.2">𝑙</ci><cn id="S7.SS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S7.SS1.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.2.m2.1c">l^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.2.m2.1d">italic_l start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm of a global image descriptor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib54" title="">54</a>]</cite>) tour frames to <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p3.3.m3.1"><semantics id="S7.SS1.p3.3.m3.1a"><mi id="S7.SS1.p3.3.m3.1.1" xref="S7.SS1.p3.3.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.3.m3.1b"><ci id="S7.SS1.p3.3.m3.1.1.cmml" xref="S7.SS1.p3.3.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.3.m3.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.3.m3.1d">italic_O</annotation></semantics></math> is determined. 2D features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib62" title="">62</a>]</cite> in <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p3.4.m4.1"><semantics id="S7.SS1.p3.4.m4.1a"><mi id="S7.SS1.p3.4.m4.1.1" xref="S7.SS1.p3.4.m4.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.4.m4.1b"><ci id="S7.SS1.p3.4.m4.1.1.cmml" xref="S7.SS1.p3.4.m4.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.4.m4.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.4.m4.1d">italic_O</annotation></semantics></math> are matched to the 2D features of each frame in <math alttext="C" class="ltx_Math" display="inline" id="S7.SS1.p3.5.m5.1"><semantics id="S7.SS1.p3.5.m5.1a"><mi id="S7.SS1.p3.5.m5.1.1" xref="S7.SS1.p3.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.5.m5.1b"><ci id="S7.SS1.p3.5.m5.1.1.cmml" xref="S7.SS1.p3.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.5.m5.1d">italic_C</annotation></semantics></math>. Using the pre-computed 2D-3D correspondences, we establish correspondences between 2D features in <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p3.6.m6.1"><semantics id="S7.SS1.p3.6.m6.1a"><mi id="S7.SS1.p3.6.m6.1.1" xref="S7.SS1.p3.6.m6.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.6.m6.1b"><ci id="S7.SS1.p3.6.m6.1.1.cmml" xref="S7.SS1.p3.6.m6.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.6.m6.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.6.m6.1d">italic_O</annotation></semantics></math> and 3D landmarks observed in the tour.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.3">Given the set of 2D-3D correspondences for each frame in <math alttext="C" class="ltx_Math" display="inline" id="S7.SS1.p4.1.m1.1"><semantics id="S7.SS1.p4.1.m1.1a"><mi id="S7.SS1.p4.1.m1.1.1" xref="S7.SS1.p4.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.1.m1.1b"><ci id="S7.SS1.p4.1.m1.1.1.cmml" xref="S7.SS1.p4.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.1.m1.1d">italic_C</annotation></semantics></math>, the pose of <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p4.2.m2.1"><semantics id="S7.SS1.p4.2.m2.1a"><mi id="S7.SS1.p4.2.m2.1.1" xref="S7.SS1.p4.2.m2.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.2.m2.1b"><ci id="S7.SS1.p4.2.m2.1.1.cmml" xref="S7.SS1.p4.2.m2.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.2.m2.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.2.m2.1d">italic_O</annotation></semantics></math> is computed by solving the corresponding Perspective-n-Point problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib55" title="">55</a>]</cite>. The pose with the most inlier 2D-3D correspondences is selected as <math alttext="T_{O}" class="ltx_Math" display="inline" id="S7.SS1.p4.3.m3.1"><semantics id="S7.SS1.p4.3.m3.1a"><msub id="S7.SS1.p4.3.m3.1.1" xref="S7.SS1.p4.3.m3.1.1.cmml"><mi id="S7.SS1.p4.3.m3.1.1.2" xref="S7.SS1.p4.3.m3.1.1.2.cmml">T</mi><mi id="S7.SS1.p4.3.m3.1.1.3" xref="S7.SS1.p4.3.m3.1.1.3.cmml">O</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.3.m3.1b"><apply id="S7.SS1.p4.3.m3.1.1.cmml" xref="S7.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS1.p4.3.m3.1.1.1.cmml" xref="S7.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S7.SS1.p4.3.m3.1.1.2.cmml" xref="S7.SS1.p4.3.m3.1.1.2">𝑇</ci><ci id="S7.SS1.p4.3.m3.1.1.3.cmml" xref="S7.SS1.p4.3.m3.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.3.m3.1c">T_{O}</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.3.m3.1d">italic_T start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="S7.F5.g1" src="extracted/5727909/sfm/errors.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Localization error: median ATE = <math alttext="0.056" class="ltx_Math" display="inline" id="S7.F5.2.m1.1"><semantics id="S7.F5.2.m1.1b"><mn id="S7.F5.2.m1.1.1" xref="S7.F5.2.m1.1.1.cmml">0.056</mn><annotation-xml encoding="MathML-Content" id="S7.F5.2.m1.1c"><cn id="S7.F5.2.m1.1.1.cmml" type="float" xref="S7.F5.2.m1.1.1">0.056</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.F5.2.m1.1d">0.056</annotation><annotation encoding="application/x-llamapun" id="S7.F5.2.m1.1e">0.056</annotation></semantics></math>m.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.2">When <math alttext="T_{O}" class="ltx_Math" display="inline" id="S7.SS1.p5.1.m1.1"><semantics id="S7.SS1.p5.1.m1.1a"><msub id="S7.SS1.p5.1.m1.1.1" xref="S7.SS1.p5.1.m1.1.1.cmml"><mi id="S7.SS1.p5.1.m1.1.1.2" xref="S7.SS1.p5.1.m1.1.1.2.cmml">T</mi><mi id="S7.SS1.p5.1.m1.1.1.3" xref="S7.SS1.p5.1.m1.1.1.3.cmml">O</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.p5.1.m1.1b"><apply id="S7.SS1.p5.1.m1.1.1.cmml" xref="S7.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS1.p5.1.m1.1.1.1.cmml" xref="S7.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S7.SS1.p5.1.m1.1.1.2.cmml" xref="S7.SS1.p5.1.m1.1.1.2">𝑇</ci><ci id="S7.SS1.p5.1.m1.1.1.3.cmml" xref="S7.SS1.p5.1.m1.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p5.1.m1.1c">T_{O}</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p5.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math> is used to determine the closest vertex on <math alttext="G" class="ltx_Math" display="inline" id="S7.SS1.p5.2.m2.1"><semantics id="S7.SS1.p5.2.m2.1a"><mi id="S7.SS1.p5.2.m2.1.1" xref="S7.SS1.p5.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p5.2.m2.1b"><ci id="S7.SS1.p5.2.m2.1.1.cmml" xref="S7.SS1.p5.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p5.2.m2.1c">G</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p5.2.m2.1d">italic_G</annotation></semantics></math>, the scale-ambiguity characteristic of monocular structure-from-motion systems is inconsequential to the high-level goal-finding policy. However, when computing the waypoint action for low-level navigation (see Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#alg1" title="Algorithm 1 ‣ 4.3 Low-level Goal Reaching using Topological Graphs ‣ 4 \method ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">1</span></a>), the scale factor is utilized to generate metrically accurate actions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p6">
<p class="ltx_p" id="S7.SS1.p6.1">We evaluated localization accuracy by comparing it to groundtruth computed by localizing unseen test images with COLMAP in a slow but accurate offline process (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.F5" title="Figure 5 ‣ 7.1 Structure-from-Motion and Hierarchical Localization ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">5</span></a>). Note that the method failed when <math alttext="O" class="ltx_Math" display="inline" id="S7.SS1.p6.1.m1.1"><semantics id="S7.SS1.p6.1.m1.1a"><mi id="S7.SS1.p6.1.m1.1.1" xref="S7.SS1.p6.1.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p6.1.m1.1b"><ci id="S7.SS1.p6.1.m1.1.1.cmml" xref="S7.SS1.p6.1.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p6.1.m1.1c">O</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p6.1.m1.1d">italic_O</annotation></semantics></math> was blurry or feature-sparse and the system was forced to fall back to the last known pose. Since we would eventually receive a feature-rich, non-blurry frame, this limitation did not affect end-to-end performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Home-Like Environment Experiment Setup</h3>
<div class="ltx_para ltx_noindent" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">A handheld Pixel 6 smartphone tour of a home-like environment was collected (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.F6" title="Figure 6 ‣ 7.2 Home-Like Environment Experiment Setup ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">6</span></a>).
The tour is 75 seconds long and contains 224 frames (3 Hz).</p>
</div>
<figure class="ltx_figure" id="S7.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="200" id="S7.F6.sf1.g1" src="extracted/5727909/mock_apt/233B7-04.jpg" width="267"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="200" id="S7.F6.sf2.g1" src="extracted/5727909/mock_apt/smartphone_tour_final.jpg" width="223"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Collection of the handheld smartphone tour (right) of a home-like environment (left).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Once the topological graph was built from the tour, <span class="ltx_ERROR undefined" id="S7.SS2.p2.1.1">\method</span> was evaluated end-to-end with 5 instructions (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.T6" title="Table 6 ‣ 7.2 Home-Like Environment Experiment Setup ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">6</span></a>) and 4 random start points.</p>
</div>
<figure class="ltx_table" id="S7.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T6.1">
<tr class="ltx_tr" id="S7.T6.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.1.1">Instruction</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T6.1.1.2">Category</td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.2.1">Where did I leave my coaster?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T6.1.2.2">Small Objects</td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S7.T6.1.3.1">
<span class="ltx_text" id="S7.T6.1.3.1.1"></span> <span class="ltx_text" id="S7.T6.1.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S7.T6.1.3.1.2.1">
<span class="ltx_tr" id="S7.T6.1.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T6.1.3.1.2.1.1.1">I want to heat up some food. Where should I go?</span></span>
<span class="ltx_tr" id="S7.T6.1.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T6.1.3.1.2.1.2.1">Where can I keep this ice cream cold?</span></span>
<span class="ltx_tr" id="S7.T6.1.3.1.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T6.1.3.1.2.1.3.1">Where can I go number 2?</span></span>
<span class="ltx_tr" id="S7.T6.1.3.1.2.1.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T6.1.3.1.2.1.4.1">Where can I eat my dinner?</span></span>
</span></span><span class="ltx_text" id="S7.T6.1.3.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S7.T6.1.3.2">Reasoning-required</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Instructions in the home-like environment</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">Even though the images from the camera are significantly different (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.F7" title="Figure 7 ‣ 7.2 Home-Like Environment Experiment Setup ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7</span></a>) from the robot’s camera with a collection trajectory independent of robot motion, we achieved 100% success rate with an SPL of 0.87.</p>
</div>
<figure class="ltx_figure" id="S7.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="200" id="S7.F7.sf1.g1" src="extracted/5727909/mock_apt/phone_mock.jpg" width="113"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="200" id="S7.F7.sf2.g1" src="extracted/5727909/mock_apt/robot_mock.png" width="250"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Images from the smartphone (left) and robot (right) cameras.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Additional Experiments</h3>
<figure class="ltx_table" id="S7.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T7.1">
<tr class="ltx_tr" id="S7.T7.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T7.1.1.1">Success Rates</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T7.1.1.2">GPT-4o</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T7.1.1.3">GPT-4o</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T7.1.1.4">Gemini 1.5 Pro</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T7.1.1.5">Gemini 1.5 Pro</td>
</tr>
<tr class="ltx_tr" id="S7.T7.1.2">
<td class="ltx_td" id="S7.T7.1.2.1"></td>
<td class="ltx_td ltx_align_center" id="S7.T7.1.2.2">Text Instruction</td>
<td class="ltx_td ltx_align_center" id="S7.T7.1.2.3">MM Instruction</td>
<td class="ltx_td ltx_align_center" id="S7.T7.1.2.4">Text Instruction</td>
<td class="ltx_td ltx_align_center" id="S7.T7.1.2.5">MM Instruction</td>
</tr>
<tr class="ltx_tr" id="S7.T7.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T7.1.3.1">Text Tour</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T7.1.3.2">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T7.1.3.3">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T7.1.3.4">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T7.1.3.5">0.20</td>
</tr>
<tr class="ltx_tr" id="S7.T7.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T7.1.4.1">Multimodal Tour</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T7.1.4.2">Exceeds token limit</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T7.1.4.3">Exceeds token limit</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T7.1.4.4">0.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T7.1.4.5"><span class="ltx_text ltx_font_bold" id="S7.T7.1.4.5.1">0.90 (Ours)</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>High-level goal finding Success Rates of multimodal user instructions as a function of VLM models and instruction representations (columns) and tour modalities (row). In MM Instructions columns, the robot’s current camera observation is fed directly into the VLMs. In Text Instructions columns, the camera observation is captioned by Gemini 1.5 Pro and the caption text is then fed into the VLMs. The text tour was captioned w/ Gemini 1.5 Pro </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">We also investigate if strictly multimodal user instructions (instructions that are nearly impossible to answer without the image) can be answered by the text modality alone.
To this end, we replace the image part of the multimodal user instructions with its caption.
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.T7" title="Table 7 ‣ 7.3 Additional Experiments ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7</span></a> shows the high-level goal reaching success rate of such setup in the Text Instruction columns compared to feeding VLMs the image (MM Instruction column).</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.p2.1.1">Multimodal user instructions requires multimodal demo tour and image instructions.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.T7" title="Table 7 ‣ 7.3 Additional Experiments ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the success rate is much higher when multimodal demo tour and image instructions are fed to the VLM (lower right corner).
Replacing the image with its caption significantly reducess success rate.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Large-Scale end-to-end sim details</h3>
<div class="ltx_para ltx_noindent" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">To further investigate <span class="ltx_ERROR undefined" id="S7.SS4.p1.1.1">\method</span>’s end-to-end performance, we use simulations to conduct large-scale evaluations.
To that end, we created a high fidelity reconstruction of the Office environment in simulation using NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib56" title="">56</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S7.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="221" id="S7.F8.sf1.g1" src="extracted/5727909/sim_tour_image.png" width="275"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S7.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="221" id="S7.F8.sf2.g1" src="extracted/5727909/real_tour_image.png" width="275"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Side-by-side comparison of the NeRF rendering and real image.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS4.p2">
<p class="ltx_p" id="S7.SS4.p2.1">The Office environment was reconstructed in simulation using ZipNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib63" title="">63</a>]</cite>.
To accomplish this, we capture 3,244 images of the office environment using a Sony A7 IV camera with a Rokinon 12mm wide-angle lens.
The capture is taken uniformly along the walkways of the office facing in all directions.
COLMAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib22" title="">22</a>]</cite> is then used to determine the image poses and camera intrinsics.
We train a NEural Radiance Field (NeRF) using the technique described in ZipNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib63" title="">63</a>]</cite>.
This NeRF is used to derive the assets needed for simulation.
To generate the collision mesh, we distill a mesh from the NeRF using the technique described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#bib.bib64" title="">64</a>]</cite>.
We then use the distilled mesh as a reference to construct the collision mesh by hand from mesh primitives using Blender.
To render realistic camera images, we align the NeRF with the simulated environment and render using the same camera intrinsics as the real robot’s cameras.
This ensures that the image distributions of the simulator and the real environment match as closely as possible.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.F8" title="Figure 8 ‣ 7.4 Large-Scale end-to-end sim details ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">8</span></a> shows a side-by-side comparison of a simulated render using NeRF and an image from the robot’s camera at the same location in the real environment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS4.p3">
<p class="ltx_p" id="S7.SS4.p3.1">We then evaluate <span class="ltx_ERROR undefined" id="S7.SS4.p3.1.1">\method</span> in simulation by randomly picking 10 instructions in both Reasoning-Free and Reasoning-Required categories each with 50 random starting poses.
Unfortunately we cannot directly user instructions in the real Office environment as the NeRF reconstruction was done a few months prior from when the demonstration tour in real was collected.
We omit the Small Objects and Multimodal instructions due to limited visual fidelity and challenges in including humans in NeRF</p>
</div>
<figure class="ltx_table" id="S7.T8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T8.1">
<tr class="ltx_tr" id="S7.T8.1.1">
<td class="ltx_td ltx_border_t" id="S7.T8.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T8.1.1.2">Reasoning-Free</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T8.1.1.3">Reasoning Required</td>
</tr>
<tr class="ltx_tr" id="S7.T8.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T8.1.2.1">High-Level Goal Finding SR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T8.1.2.2">90%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T8.1.2.3">90%</td>
</tr>
<tr class="ltx_tr" id="S7.T8.1.3">
<td class="ltx_td ltx_align_center" id="S7.T8.1.3.1">Low-Level Goal Reaching SR</td>
<td class="ltx_td ltx_align_center" id="S7.T8.1.3.2">100%</td>
<td class="ltx_td ltx_align_center" id="S7.T8.1.3.3">100%</td>
</tr>
<tr class="ltx_tr" id="S7.T8.1.4">
<td class="ltx_td ltx_align_center" id="S7.T8.1.4.1">End-to-end SR</td>
<td class="ltx_td ltx_align_center" id="S7.T8.1.4.2">90%</td>
<td class="ltx_td ltx_align_center" id="S7.T8.1.4.3">90%</td>
</tr>
<tr class="ltx_tr" id="S7.T8.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T8.1.5.1">SPL</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T8.1.5.2">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T8.1.5.3">0.84</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>End-to-end navigation Success Rate (SR) and SPL of various user instruction types in the <span class="ltx_text ltx_font_italic" id="S7.T8.3.1">simulated</span> Office environment.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Full VLM intermediate output in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.F3" title="Figure 3 ‣ 5.2 RQ2: Long-context VLM outperforms alternatives on high level goal finding ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">3</span></a>
</h3>
<figure class="ltx_table" id="S7.T9">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T9.1">
<tr class="ltx_tr" id="S7.T9.1.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S7.T9.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.1.1.1">
<span class="ltx_p" id="S7.T9.1.1.1.1.1">GPT-4V</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S7.T9.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.1.2.1">
<span class="ltx_p" id="S7.T9.1.1.2.1.1">GPT-4o</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S7.T9.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.1.3.1">
<span class="ltx_p" id="S7.T9.1.1.3.1.1">Text Only <span class="ltx_ERROR undefined" id="S7.T9.1.1.3.1.1.1">\method</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S7.T9.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.1.4.1"><span class="ltx_ERROR undefined" id="S7.T9.1.1.4.1.1">\method</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T9.1.2">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S7.T9.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.2.1.1">
<span class="ltx_p" id="S7.T9.1.2.1.1.1">It appears you are interested in finding more Coca-Cola Zero Sugar cans, specifically the ones with the Marvel character design. To assist you in locating more of these cans within the building, I would need to navigate to the area where beverages or pantry supplies are stored. Based on the images provided, there isn’t a specific frame that shows a location where beverages are stored or a pantry area. However, common places to look for additional cans of soda in an office or building environment would be a kitchen, break room, or storage area where supplies are kept. If you can provide more context or direct me to a specific area within the building where beverages are typically stored, I can attempt to find the closest frame that matches that description. Otherwise, I recommend checking common areas where refreshments are usually available.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S7.T9.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.2.2.1">
<span class="ltx_p" id="S7.T9.1.2.2.1.1">To find more of the Coca-Cola cans, you should navigate to the closest frame in the tour video that shows the area where the cans are likely located. The current frame you see is Frame 945, which shows a workspace area. The closest frame in the tour video to this location is Frame 935. This frame shows a workspace area with desks and equipment, which is similar to the current frame you see. Navigate to Frame 935 to find more of the Coca-Cola cans.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S7.T9.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.2.3.1">
<span class="ltx_p" id="S7.T9.1.2.3.1.1">This is a tricky one! As a robot, I can’t really understand what “this” refers to in your request. It could be the soda cans, the office setting, or something else entirely. To help me understand, could you please be more specific? For example, you could say:* “I want to see more pictures of soda cans.”* “I want to see more pictures of offices.” * “I want to see more pictures of whatever is in this image”. “Once I understand what you’re looking for, I can try to find the closest matching frame from my database”.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S7.T9.1.2.4">
<span class="ltx_inline-block ltx_align_top" id="S7.T9.1.2.4.1">
<span class="ltx_p" id="S7.T9.1.2.4.1.1">Frame 222 is the closest frame as it shows the same refrigerator</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Full VLM intermediate output wrt to the multimodal user instruction shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S5.F3" title="Figure 3 ‣ 5.2 RQ2: Long-context VLM outperforms alternatives on high level goal finding ‣ 5 Experiments ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">3</span></a></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6 </span>Prompt for direct VLM waypoint actions output</h3>
<div class="ltx_para ltx_noindent" id="S7.SS6.p1">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="S7.SS6.p1.1" style="background-color:#F5F5F5;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,WW91IGFyZSBhIHJvYm90IG9wZXJhdGluZyBpbiBhIGJ1aWxkaW5nIGFuZCB5b3VyIHRhc2sgaXMgdG8gcmVzcG9uZCB0byB0aGUgdXNlcgpjb21tYW5kIGFib3V0IGdvaW5nIHRvIGEgc3BlY2lmaWMgbG9jYXRpb24gYnkgZmluZGluZyB0aGUgY2xvc2VzdCBmcmFtZSBpbiB0aGUKdG91ciB2aWRlbyB0byBuYXZpZ2F0ZSB0by4KVGhlc2UgZnJhbWVzIGFyZSBmcm9tIHRoZSB0b3VyIG9mIHRoZSBidWlsZGluZyBsYXN0IHllYXIuCltGcmFtZSAxIEltYWdlICRmXzEkXQpGcmFtZSAxLiBbRnJhbWUgbmFycmF0aXZlICRuXzEkXQouLi4KW0ZyYW1lIGsgSW1hZ2UgJGZfayRdCkZyYW1lIGsuIFtGcmFtZSBuYXJyYXRpdmUgJG5fayRdClRoaXMgaW1hZ2UgaXMgd2hhdCB5b3Ugc2VlIG5vdy4gWW91IG1heSBvciBtYXkgbm90IHNlZSB0aGUgdXNlciBpbiB0aGlzIGltYWdlLgpbSW1hZ2UgSW5zdHJ1Y3Rpb24gSV0KVGhlIHVzZXIgc2F5czogV2hlcmUgc2hvdWxkIEkgcmV0dXJuIHRoaXM/CkNvdWxkIHlvdSBzZWxlY3QgYW5kIGFuc3dlciB0aGUgbW9zdCBhcHByb3ByaWF0ZSBhY3Rpb24gdG8gdGFrZSBub3cgYW1vbmcKJ2xlZnQnLCAnZm9yd2FyZCcgYW5kICdyaWdodCcsIHdoaWNoIGNvcnJlc3BvbmQgdG8gcmVzcGVjdGl2ZWx5PyBBbnN3ZXI6">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.7">robot</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.9">operating</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.11">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.13">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.15">building</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.17">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.19">your</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.21">task</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.23">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.25">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.27">respond</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.29">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.31">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.33">user</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.1">command</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.3">about</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.5">going</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.9">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.11">specific</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.13">location</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.15">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.17">finding</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.19">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.21">closest</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.23">frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.25">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.27">the</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.1">tour</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.3">video</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.5">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.7">navigate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.9">to</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.10">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.1">These</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.3">frames</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.5">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.7">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.9">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.11">tour</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.15">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.17">building</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.19">last</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.21">year</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.22">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx18.1">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.2">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.3"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4">1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.6">Image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.8">$f_1$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.9">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.1">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.3">1.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.5">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.6">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.8">narrative</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.10">$n_1$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.11">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx20.1">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx21.1">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.2">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.4">k</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.6">Image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.8">$f_k$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.9">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.1">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.3">k</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.4">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.6">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.7">Frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.9">narrative</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.11">$n_k$</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.12">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.1">This</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.3">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.5">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.7">what</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.9">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.11">see</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.13">now</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.14">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.16">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.18">may</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.20">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.22">may</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.24">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.26">see</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.28">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.30">user</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.32">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.34">this</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.36">image</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.37">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx24.1">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.2">Image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.4">Instruction</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.6">I</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.7">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx25">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.1">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.3">user</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.5">says</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.6">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.8">Where</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.10">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.12">I</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.14">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.16">this</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.17">?</span>
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.1">Could</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.3">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.5">select</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.7">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.9">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.11">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.13">most</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.15">appropriate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.17">action</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.19">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.21">take</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.23">now</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.25">among</span>
</div>
<div class="ltx_listingline" id="lstnumberx27">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx27.1">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.2">left</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.3">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.5">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.6">forward</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.7">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.9">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.11">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.12">right</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.13">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.15">which</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.17">correspond</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.19">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.21">respectively</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.22">?</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.24">Answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.25">:</span>
</div>
</div>
</div>
</section>
<section class="ltx_subsection" id="S7.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7 </span>User Instructions tested in the Office environment</h3>
<div class="ltx_para ltx_noindent" id="S7.SS7.p1">
<p class="ltx_p" id="S7.SS7.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS7.p1.1.1">Reasoning-Free Instructions.</span></p>
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1">Can you take me to the building map?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1">Where can I find a ladder?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1">Take me to the exit</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S7.I1.i4.p1">
<p class="ltx_p" id="S7.I1.i4.p1.1">Where can I find some paper cups?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S7.I1.i5.p1">
<p class="ltx_p" id="S7.I1.i5.p1.1">gray trash can.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S7.I1.i6.p1">
<p class="ltx_p" id="S7.I1.i6.p1.1">Take me to robot number 109.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S7.I1.i7.p1">
<p class="ltx_p" id="S7.I1.i7.p1.1">Take me to a blue area.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S7.I1.i8.p1">
<p class="ltx_p" id="S7.I1.i8.p1.1">I want to borrow my friend’s scooter, can you take me to it?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="S7.I1.i9.p1">
<p class="ltx_p" id="S7.I1.i9.p1.1">Take me to a conference room with a double door.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="S7.I1.i10.p1">
<p class="ltx_p" id="S7.I1.i10.p1.1">I need a tripod, where can I find it in this office?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">11.</span>
<div class="ltx_para" id="S7.I1.i11.p1">
<p class="ltx_p" id="S7.I1.i11.p1.1">Take me to a whiteboard.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">12.</span>
<div class="ltx_para" id="S7.I1.i12.p1">
<p class="ltx_p" id="S7.I1.i12.p1.1">Where are the gray cabinets again?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">13.</span>
<div class="ltx_para" id="S7.I1.i13.p1">
<p class="ltx_p" id="S7.I1.i13.p1.1">I heard there’s a cool dark-backgrounded poster, where is it?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i14" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">14.</span>
<div class="ltx_para" id="S7.I1.i14.p1">
<p class="ltx_p" id="S7.I1.i14.p1.1">where can I find a long wooden bench?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i15" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">15.</span>
<div class="ltx_para" id="S7.I1.i15.p1">
<p class="ltx_p" id="S7.I1.i15.p1.1">Take me to a two-paned door</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i16" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">16.</span>
<div class="ltx_para" id="S7.I1.i16.p1">
<p class="ltx_p" id="S7.I1.i16.p1.1">I’m Lewis, take me to a temp desk please.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i17" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">17.</span>
<div class="ltx_para" id="S7.I1.i17.p1">
<p class="ltx_p" id="S7.I1.i17.p1.1">Hi robot, I’m visiting, can you take me to a temp desk?</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i18" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">18.</span>
<div class="ltx_para" id="S7.I1.i18.p1">
<p class="ltx_p" id="S7.I1.i18.p1.1">Take me to a white shelf</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i19" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">19.</span>
<div class="ltx_para" id="S7.I1.i19.p1">
<p class="ltx_p" id="S7.I1.i19.p1.1">Take me to a plant</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i20" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">20.</span>
<div class="ltx_para ltx_noindent" id="S7.I1.i20.p1">
<p class="ltx_p" id="S7.I1.i20.p1.1">where can I find a moving box?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS7.p2">
<p class="ltx_p" id="S7.SS7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS7.p2.1.1">Reasoning-Required Instructions.</span></p>
<ol class="ltx_enumerate" id="S7.I2">
<li class="ltx_item" id="S7.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I2.i1.p1">
<p class="ltx_p" id="S7.I2.i1.p1.1">There is a fire, where should I find tools to fight the fire?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I2.i2.p1">
<p class="ltx_p" id="S7.I2.i2.p1.1">I’m thirsty.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I2.i3.p1">
<p class="ltx_p" id="S7.I2.i3.p1.1">I’m here to water things, please guide take me to them.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S7.I2.i4.p1">
<p class="ltx_p" id="S7.I2.i4.p1.1">Help me dispose of this cardboard box.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S7.I2.i5.p1">
<p class="ltx_p" id="S7.I2.i5.p1.1">Take me to a room with a closed door.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S7.I2.i6.p1">
<p class="ltx_p" id="S7.I2.i6.p1.1">I want to store something out of sight from public eyes. Where should I go?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S7.I2.i7.p1">
<p class="ltx_p" id="S7.I2.i7.p1.1">I left my drink on a cart, can you take me to it?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S7.I2.i8.p1">
<p class="ltx_p" id="S7.I2.i8.p1.1">Can you take me upstairs?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="S7.I2.i9.p1">
<p class="ltx_p" id="S7.I2.i9.p1.1">I need to charge my phone, please help.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="S7.I2.i10.p1">
<p class="ltx_p" id="S7.I2.i10.p1.1">I heard there is a place to see lots of robots?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">11.</span>
<div class="ltx_para" id="S7.I2.i11.p1">
<p class="ltx_p" id="S7.I2.i11.p1.1">I need to sit down.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">12.</span>
<div class="ltx_para" id="S7.I2.i12.p1">
<p class="ltx_p" id="S7.I2.i12.p1.1">Can you take me somewhere to lie down?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">13.</span>
<div class="ltx_para" id="S7.I2.i13.p1">
<p class="ltx_p" id="S7.I2.i13.p1.1">Where can I find something cold?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i14" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">14.</span>
<div class="ltx_para" id="S7.I2.i14.p1">
<p class="ltx_p" id="S7.I2.i14.p1.1">I’m tired. Where can I rest?</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i15" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">15.</span>
<div class="ltx_para ltx_noindent" id="S7.I2.i15.p1">
<p class="ltx_p" id="S7.I2.i15.p1.1">I want to draw something.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS7.p3">
<p class="ltx_p" id="S7.SS7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS7.p3.1.1">Small Objects Instructions.</span></p>
<ol class="ltx_enumerate" id="S7.I3">
<li class="ltx_item" id="S7.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I3.i1.p1">
<p class="ltx_p" id="S7.I3.i1.p1.1">Where is the Jackery portable power station?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I3.i2.p1">
<p class="ltx_p" id="S7.I3.i2.p1.1">where is the bench with a bag of chips on it?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I3.i3.p1">
<p class="ltx_p" id="S7.I3.i3.p1.1">where can I find a fire extinguisher?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S7.I3.i4.p1">
<p class="ltx_p" id="S7.I3.i4.p1.1">Where can I borrow a hand sanitizer?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S7.I3.i5.p1">
<p class="ltx_p" id="S7.I3.i5.p1.1">I heard there is a cute tiny traffic cone, where is it?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S7.I3.i6.p1">
<p class="ltx_p" id="S7.I3.i6.p1.1">I need a xbox controller.</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S7.I3.i7.p1">
<p class="ltx_p" id="S7.I3.i7.p1.1">Did you see my white water bottle?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S7.I3.i8.p1">
<p class="ltx_p" id="S7.I3.i8.p1.1">Where can I find a fire alarm switch?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="S7.I3.i9.p1">
<p class="ltx_p" id="S7.I3.i9.p1.1">Can you help me find my cat mask?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="S7.I3.i10.p1">
<p class="ltx_p" id="S7.I3.i10.p1.1">My friend told me to get his moving box under his desk, can you help me find it?</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">11.</span>
<div class="ltx_para" id="S7.I3.i11.p1">
<p class="ltx_p" id="S7.I3.i11.p1.1">take me to the tombstone I heard so much about.</p>
</div>
</li>
<li class="ltx_item" id="S7.I3.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">12.</span>
<div class="ltx_para ltx_noindent" id="S7.I3.i12.p1">
<p class="ltx_p" id="S7.I3.i12.p1.1">where can I find a toy cart?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS7.p4">
<p class="ltx_p" id="S7.SS7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS7.p4.1.1">Multimodal Instructions.</span> See Table <a class="ltx_ref" href="https://arxiv.org/html/2407.07775v2#S7.T10" title="Table 10 ‣ 7.7 User Instructions tested in the Office environment ‣ 7 Appendix ‣ \method: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs"><span class="ltx_text ltx_ref_tag">10</span></a> below.</p>
</div>
<figure class="ltx_table" id="S7.T10">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T10.9">
<tr class="ltx_tr" id="S7.T10.9.10">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T10.9.10.1" style="padding-top:15pt;padding-bottom:15pt;">#</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T10.9.10.2" style="padding-top:15pt;padding-bottom:15pt;">Text</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T10.9.10.3" style="padding-top:15pt;padding-bottom:15pt;">Image</td>
</tr>
<tr class="ltx_tr" id="S7.T10.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T10.1.1.2" style="padding-top:15pt;padding-bottom:15pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T10.1.1.3" style="padding-top:15pt;padding-bottom:15pt;">Where should I return this?</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T10.1.1.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.1.1.1.g1" src="extracted/5727909/mm_question_set_v1/toy_box_final.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.2.2">
<td class="ltx_td ltx_align_center" id="S7.T10.2.2.2" style="padding-top:15pt;padding-bottom:15pt;">2</td>
<td class="ltx_td ltx_align_center" id="S7.T10.2.2.3" style="padding-top:15pt;padding-bottom:15pt;">I don’t want this anymore. Can you help me?</td>
<td class="ltx_td ltx_align_center" id="S7.T10.2.2.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.2.2.1.g1" src="extracted/5727909/mm_question_set_v1/can_in_hand_final.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.3.3">
<td class="ltx_td ltx_align_center" id="S7.T10.3.3.2" style="padding-top:15pt;padding-bottom:15pt;">3</td>
<td class="ltx_td ltx_align_center" id="S7.T10.3.3.3" style="padding-top:15pt;padding-bottom:15pt;">Where can I charge this?</td>
<td class="ltx_td ltx_align_center" id="S7.T10.3.3.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.3.3.1.g1" src="extracted/5727909/mm_question_set_v1/iphone_in_hand.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.4.4">
<td class="ltx_td ltx_align_center" id="S7.T10.4.4.2" style="padding-top:15pt;padding-bottom:15pt;">4</td>
<td class="ltx_td ltx_align_center" id="S7.T10.4.4.3" style="padding-top:15pt;padding-bottom:15pt;">I want more of this.</td>
<td class="ltx_td ltx_align_center" id="S7.T10.4.4.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.4.4.1.g1" src="extracted/5727909/mm_question_set_v1/cokes.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.5.5">
<td class="ltx_td ltx_align_center" id="S7.T10.5.5.2" style="padding-top:15pt;padding-bottom:15pt;">5</td>
<td class="ltx_td ltx_align_center" id="S7.T10.5.5.3" style="padding-top:15pt;padding-bottom:15pt;">Can you follow the directions on the whiteboard?</td>
<td class="ltx_td ltx_align_center" id="S7.T10.5.5.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="96" id="S7.T10.5.5.1.g1" src="extracted/5727909/mm_question_set_v1/dir_on_whiteboard.png" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.6.6">
<td class="ltx_td ltx_align_center" id="S7.T10.6.6.2" style="padding-top:15pt;padding-bottom:15pt;">6</td>
<td class="ltx_td ltx_align_center" id="S7.T10.6.6.3" style="padding-top:15pt;padding-bottom:15pt;">I don’t think this is supposed to be here, where should it go?</td>
<td class="ltx_td ltx_align_center" id="S7.T10.6.6.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.6.6.1.g1" src="extracted/5727909/mm_question_set_v1/trash_can_on_wheels_final.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.7.7">
<td class="ltx_td ltx_align_center" id="S7.T10.7.7.2" style="padding-top:15pt;padding-bottom:15pt;">7</td>
<td class="ltx_td ltx_align_center" id="S7.T10.7.7.3" style="padding-top:15pt;padding-bottom:15pt;">Where can I get something to clean this?</td>
<td class="ltx_td ltx_align_center" id="S7.T10.7.7.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.7.7.1.g1" src="extracted/5727909/mm_question_set_v1/busy_whiteboard_final.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.8.8">
<td class="ltx_td ltx_align_center" id="S7.T10.8.8.2" style="padding-top:15pt;padding-bottom:15pt;">8</td>
<td class="ltx_td ltx_align_center" id="S7.T10.8.8.3" style="padding-top:15pt;padding-bottom:15pt;">Where can I use this?</td>
<td class="ltx_td ltx_align_center" id="S7.T10.8.8.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="91" id="S7.T10.8.8.1.g1" src="extracted/5727909/mm_question_set_v1/markers.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S7.T10.9.9">
<td class="ltx_td ltx_align_center" id="S7.T10.9.9.2" style="padding-top:15pt;padding-bottom:15pt;">9</td>
<td class="ltx_td ltx_align_center" id="S7.T10.9.9.3" style="padding-top:15pt;padding-bottom:15pt;">Where can I see the most amount of this?</td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_bb ltx_border_b" id="S7.T10.9.9.1" rowspan="2" style="padding-top:15pt;padding-bottom:15pt;"><span class="ltx_text" id="S7.T10.9.9.1.1"><span class="ltx_text" id="S7.T10.9.9.1.1.2"></span> <span class="ltx_text" id="S7.T10.9.9.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S7.T10.9.9.1.1.1.1">
<span class="ltx_tr" id="S7.T10.9.9.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T10.9.9.1.1.1.1.1.1" style="padding-top:15pt;padding-bottom:15pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="159" id="S7.T10.9.9.1.1.1.1.1.1.g1" src="extracted/5727909/mm_question_set_v1/meta.jpg" width="120"/></span></span>
</span></span> <span class="ltx_text" id="S7.T10.9.9.1.1.3"></span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T10.9.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S7.T10.9.11.1" style="padding-bottom:17.22217pt;padding-top:15pt;padding-bottom:15pt;">10</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b" id="S7.T10.9.11.2" style="padding-bottom:17.22217pt;padding-top:15pt;padding-bottom:15pt;">Can you take me somewhere to NOT see this?</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Multimodal Instructions.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section><div about="" class="ltx_rdf" content="Anonymous Submission" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="vision-language navigation, multimodal foundation models, long-context reasoning" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="Proceedings of the 8th Conference on Robot Learning (CoRL 2024)" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jul 12 14:32:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
